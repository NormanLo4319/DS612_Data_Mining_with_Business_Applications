# Fitting a Prunning Tree Model
# Using 5-fold and 10-fold cross validation to find the best size of tree
cvTree <- cv.tree(treeFit, FUN=prune.misclass, K=5)
cvTree
cv10_Tree <- cv.tree(treeFit, FUN=prune.misclass, K=10)
cv10_Tree
# 5-fold Cross Validation
cv5_Tree <- cv.tree(treeFit, FUN=prune.misclass, K=5)
cv5_Tree
# Plot the result
plot(cvTree$size, cvTree$dev, type="b", col="blue")
# 10-fold Cross Validation
cv10_Tree <- cv.tree(treeFit, FUN=prune.misclass, K=10)
cv10_Tree
# Plot the result
lines(cv_10Tree$size, cv_10Tree$dev, type="b", col="red")
# Plot the result
lines(cv10_Tree$size, cv10_Tree$dev, type="b", col="red")
legend(15, 120, legend=c("5-Fold CV", "10-Fold CV"),
col=c("blue", "red"), lty=1)
legend(15, 120, legend=c("5-Fold CV", "10-Fold CV"),
col=c("blue", "red"), lty=1)
legend(12, 150, legend=c("5-Fold CV", "10-Fold CV"),
col=c("blue", "red"), lty=1)
# Plot the result
plot(cvTree$size, cvTree$dev, type="b", col="blue")
# Plot the result
lines(cv10_Tree$size, cv10_Tree$dev, type="b", col="red")
legend(12, 150, legend=c("5-Fold CV", "10-Fold CV"),
col=c("blue", "red"), lty=1)
# Fitting the best prunning tree
prunedFit = prune.misclass(treeFit, best=3)
prunedFit
# Plot the Pruned Tree
plot(prunedFit)
text(prunedFit, pretty=0)
# Check the test error rate
prunedPred <- predict(prunedFit, testData, type="class")
table(prunedPred,testOutcome)
mean(prunedPred != testOutcome)
head(Boston)
# Bagging
# Again, bagging is just a special case of random forest, which uses all the avaialbe
# predictors for tree splitting from the bootstrap data sets
bagTreeFit <- randomForest(homePrice~., data=trainData, mtry=13, importance=TRUE)
bagTreeFit
summary(bagTreeFit)
# Check the most important predictor from the data
importance(bag.boston)
# Check the most important predictor from the data
importance(bagTreeFit)
# Plot the importance
varImpPlot(bagTreeFit)
# Make prediction with the testing data set
bagTreePred <- predict(bagTreeFit, newdata=testData)
table(bagTreePred,testOutcome)
mean(bagTreePred != testOutcome)
# Spliting the data into training and testing set,
set.seed(1)
n = nrow(boston)
n # that is 506
train = sample(1:n, 300)
test = -train
trainData = boston[train,]
testData  = boston[test,]
testOutcome = boston$homePrice[test]
# Fitting a Classification Tree Model
treeFit <- tree(homePrice~., data=boston)
summary(treeFit)
# Plot the tree diagram
plot(treeFit)
text(treeFit, pretty=0)
# See each of the split from the tree model
treeFit
# Check the test error rate
treePred <- predict(treeFit, testData, type="class")
table(treePred,testOutcome)
mean(treePred != testOutcome)  # 0.0566
# 5-fold Cross Validation
cv5_Tree <- cv.tree(treeFit, FUN=prune.misclass, K=5)
cv5_Tree
# Plot the result
plot(cvTree$size, cvTree$dev, type="b", col="blue")
# 10-fold Cross Validation
cv10_Tree <- cv.tree(treeFit, FUN=prune.misclass, K=10)
cv10_Tree
# Plot the result
lines(cv10_Tree$size, cv10_Tree$dev, type="b", col="red")
legend(12, 150, legend=c("5-Fold CV", "10-Fold CV"),
col=c("blue", "red"), lty=1)
# Fitting the best prunning tree
prunedFit = prune.misclass(treeFit, best=3)
prunedFit
# Plot the Pruned Tree
plot(prunedFit)
text(prunedFit, pretty=0)
# Check the test error rate
prunedPred <- predict(prunedFit, testData, type="class")
table(prunedPred,testOutcome)
mean(prunedPred != testOutcome)  # 0.10377
# Bagging
# Again, bagging is just a special case of random forest, which uses all the avaialbe
# predictors for tree splitting from the bootstrap data sets
bagTreeFit <- randomForest(homePrice~., data=trainData, mtry=13, importance=TRUE)
summary(bagTreeFit)
# Check the most important predictor from the data
importance(bagTreeFit)
# Plot the importance
varImpPlot(bagTreeFit)
# Check the testing error rate
bagTreePred <- predict(bagTreeFit, newdata=testData)
table(bagTreePred,testOutcome)
mean(bagTreePred != testOutcome)
# Bagging
# Again, bagging is just a special case of random forest, which uses all the avaialbe
# predictors for tree splitting from the bootstrap data sets
bagTreeFit <- randomForest(homePrice~., data=trainData, mtry=13, importance=TRUE, ntree=1000)
summary(bagTreeFit)
# Check the most important predictor from the data
importance(bagTreeFit)
# Plot the importance
varImpPlot(bagTreeFit)
# Check the testing error rate
bagTreePred <- predict(bagTreeFit, newdata=testData)
table(bagTreePred,testOutcome)
mean(bagTreePred != testOutcome)  # 0.11165
# Bagging
# Again, bagging is just a special case of random forest, which uses all the avaialbe
# predictors for tree splitting from the bootstrap data sets
bagTreeFit <- randomForest(homePrice~., data=trainData, mtry=13, importance=TRUE, ntree=200)
summary(bagTreeFit)
# Check the most important predictor from the data
importance(bagTreeFit)
# Plot the importance
varImpPlot(bagTreeFit)
# Check the testing error rate
bagTreePred <- predict(bagTreeFit, newdata=testData)
table(bagTreePred,testOutcome)
mean(bagTreePred != testOutcome)  # 0.11165
# Bagging
# Again, bagging is just a special case of random forest, which uses all the avaialbe
# predictors for tree splitting from the bootstrap data sets
bagTreeFit <- randomForest(homePrice~., data=trainData, mtry=13, importance=TRUE, ntree=500)
summary(bagTreeFit)
# Check the testing error rate
bagTreePred <- predict(bagTreeFit, newdata=testData)
table(bagTreePred,testOutcome)
mean(bagTreePred != testOutcome)  # 0.11165
# Random Forest
# Now, let's see what is a good number of predictors to use for spliting the tree
rfError = rep(0,13)
for(d in 1:13){
rfTreeFit = randomForest(homePrice~., data=trainData, mtry=d, importance=TRUE)
rfTreePred = predict(rfTreeFit, newdata=testData)
rfError[d] = mean(rfTreePred != TestOutcome)
}
for(d in 1:13){
rfTreeFit = randomForest(homePrice~., data=trainData, mtry=d, importance=TRUE)
rfTreePred = predict(rfTreeFit, newdata=testData)
rfError[d] = mean(rfTreePred != testOutcome)
}
# Plot the MSE for each size of ntry
plot(MTRY, rfError, type="b", col="blue")
MTRY = c(1:13)
# Plot the MSE for each size of ntry
plot(MTRY, rfError, type="b", col="blue")
data.frame(MTRY, rfError)
# Use maximum 3 predictors for spliting trees from bootstrap data set
rfTreeFit = randomForest(homePrice~., data=trainData, mtry=3, importance=TRUE)
summary(rfTreeFit)
# Check the most important predictor from the data
importance(rfTreeFit)
# Plot the importance
varImpPlot(rfTreeFit)
# Check the testing error rate
rfTreePred <- predict(rfTreeFit, newdata=testData)
table(rfTreePred,testOutcome)
mean(rfTreePred != testOutcome)
library(gbm)
# Boosting
# Building a forest of tree with stump, where depth=1.
boostTreeFit <- gbm(homePrice~., data=trainData, distribution="bernoulli",
n.trees=5000, interaction.depth=1, shrinkage=0.01)
# Boosting
# Building a forest of tree with "stumps", where depth=1.
boston$price <- ifelse(trainData$homePrice == "High", 1, 0)
# Boosting
# Building a forest of tree with "stumps", where depth=1.
trainData$price <- ifelse(trainData$homePrice == "High", 1, 0)
testData$price <- ifelse(testData$homePrice == "High", 1, 0)
boostTreeFit <- gbm(homePrice~., data=trainData, distribution="bernoulli",
n.trees=5000, interaction.depth=1, shrinkage=0.01)
boostTreeFit <- gbm(price~.-homePrice, data=trainData, distribution="bernoulli",
n.trees=5000, interaction.depth=1, shrinkage=0.01)
boostTreeFit
summary(boostTreeFit)
# Check the testing error rate
testOutcome2 = testData$price
boostTreePred <- predict(boostTreeFit, newdata=testData, n.trees=5000)
table(boostTreePred,testOutcome2)
mean(boostTreePred != testOutcome2)
boostTreePred
boostTreeFit <- gbm(homePrice~.price, data=trainData, distribution="bernoulli",
n.trees=5000, interaction.depth=1, shrinkage=0.01)
boostTreeFit <- gbm(homePrice~.-price, data=trainData, distribution="bernoulli",
n.trees=5000, interaction.depth=1, shrinkage=0.01)
testOutcome2
boostTreePred <- ifelse(boostTreePred > 0, 1, 0)
table(boostTreePred,testOutcome2)
mean(boostTreePred != testOutcome2)
# Validation Set Approach
require(ISLR)
require(boot)
attach(Auto)
?Auto
pairs(Auto)
# Splitting data to 300 training observations and 92 testing observations
set.seed(1)
train <- sample(dim(Auto)[1], 300)
train
test <- -train
trainData <- Auto[train,]
testData  <- Auto[test,]
trainOutcome <- Auto$mpg[train]
testOutcome <- Auto$mpg[test]
fit1 <- lm(mpg~horsepower, data=Auto, subset=train)
summary(fit1)
fit2 <- lm(mpg~horsepower+weight, data=trainData)
summary(fit2)
fit3 <- lm(mpg~horsepower+weight+displacement, data=Auto[train,])
summary(fit3)
# Model 1 MSEs
# Training MSE
fit1_trainMSE <- mean((Auto$mpg-predict(fit1, Auto))[train]^2)
fit1_trainMSE
# Testing MSE
fit1_testMSE <- mean((Auto$mpg-predict(fit1, Auto))[-train]^2)
fit1_testMSE
# Model 2 MSEs
# Training MSE
fit2_trainPred <- predict(fit2, newdata=trainData)
fit2_trainMSE <- mean((fit2_trainPred - trainOutcome)^2)
fit2_trainMSE
# Testing MSE
fit2_testPred <- predict(fit2, newdata=testData)
fit2_testMSE <- mean((fit2_testPred - testOutcome)^2)
fit2_testMSE
# Model 3 MSEs
# Training MSE
fit3_trainPred <- predict(fit3, Auto)[train]
fit3_trainMSE <- mean((fit3_trainPred - trainOutcome)^2)
fit3_trainMSE
# Test MSE
fit3_testPred <- predict(fit3, Auto)[-train]
fit3_testMSE <- mean((fit3_testPred - testOutcome)^2)
fit3_testMSE
# Create data frame to show result
models <- c("fit1", "fit2", "fit3")
x <- c(1, 2, 3)
trainMSEs <- c(fit1_trainMSE, fit2_trainMSE, fit3_trainMSE)
testMSEs <- c(fit1_testMSE, fit2_testMSE, fit3_testMSE)
results <- data.frame(models, trainMSEs, testMSEs)
results
# Plot the training and testing MSEs
plot(x, testMSEs, type="b", col="blue")
lines(x, trainMSEs, type="b", col="red")
legend(2.6, 24, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"),
lty=1)
# LOOCV Approach
fit1 <- glm(mpg~horsepower, data=Auto)
summary(fit1)
fit2 <- glm(mpg~horsepower+weight, data=Auto)
summary(fit2)
fit3 <- glm(mpg~horsepower+weight+displacement, data=Auto)
summary(fit3)
# Calculate the LOOCV MSE for each model
# Model 1 LOOCV MSE
fit1_loocv <- cv.glm(Auto, fit1)$delta[1]
fit1_loocv
# Model 2 LOOCV MSE
loocv = function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
fit2_loocv <- loocv(fit2)
fit2_loocv
# Model 3 LOOCV MSE
fit3_loocv <- loocv(fit3)
fit3_loocv
loocv_MSEs <- c(fit1_loocv, fit2_loocv, fit3_loocv)
loocv_results <- data.frame(models, loocv_MSEs)
loocv_results
# Adding the LOOCV MSEs to the previous graph
lines(x, loocv_MSEs, type="b", col="green")
legend(2.5, 24, legend=c("testMSE", "trainMSE", "LOOCVMSE"),
col=c("blue", "red", "green"),
lty=1)
# K-Folds Cross-Validation
# 5-Fold Cross-Validation
# Model 1 MSE
fit1_5fold <- cv.glm(Auto, fit1, K=5)$delta[1]
fit1_5fold
# Model 1 MSE
fit2_5fold <- cv.glm(Auto, fit2, K=5)$delta[1]
fit2_5fold
# Model 1 MSE
fit3_5fold <- cv.glm(Auto, fit3, K=5)$delta[1]
fit3_5fold
# 10-Fold Cross-Validation
# Model 1 MSE
fit1_10fold <- cv.glm(Auto, fit1, K=10)$delta[1]
fit1_10fold
# Model 1 MSE
fit2_10fold <- cv.glm(Auto, fit2, K=10)$delta[1]
fit2_10fold
# Model 1 MSE
fit3_10fold <- cv.glm(Auto, fit3, K=10)$delta[1]
fit3_10fold
fold5_MSEs <- c(fit1_5fold, fit2_5fold, fit3_5fold)
fold10_MSEs <- c(fit1_10fold, fit2_10fold, fit3_10fold)
# Store results into a data frame
fold_results <- data.frame(models, fold5_MSEs, fold10_MSEs)
fold_results
# Plot the results to the same graph
lines(x, fold5_MSEs, type="b", col="purple")
lines(x, fold10_MSEs, type="b", col="brown")
legend(2.5, 24, legend=c("testMSE", "trainMSE", "LOOCVMSE", "5-Fold MSE", "10-Fold MSE"),
col=c("red", "blue", "green", "purple", 'brown'),
lty=1)
# Objective: Practice using tree model for prediction
rm(list=ls(all=TRUE))
library(randomForest)
library(ISLR)
library(tree)
library(gbm)
?Boston
library(MASS)
?Boston
head(Boston)
attach(Boston)
# Create a binary variable for high and low medv
range(medv)
homePrice <- ifelse(medv<=24, "Low", "High")
boston <- data.frame(Boston, homePrice)
boston <- boston[,-14]
head(boston)
# Spliting the data into training and testing set,
set.seed(1)
n = nrow(boston)
train = sample(1:n, 300)
test = -train
trainData = boston[train,]
testData  = boston[test,]
testOutcome = boston$homePrice[test]
# Fitting a Classification Tree Model
treeFit <- tree(homePrice~., data=trainData)
summary(treeFit)
# Plot the tree diagram
plot(treeFit)
text(treeFit, pretty=0)
# See each of the split from the tree model
treeFit
# Check the test error rate
treePred <- predict(treeFit, testData, type="class")
table(treePred,testOutcome)
mean(treePred != testOutcome)  # 0.04854
# 5-fold Cross Validation
cv5_Tree <- cv.tree(treeFit, FUN=prune.misclass, K=5)
cv5_Tree
# Plot the result
plot(cvTree$size, cvTree$dev, type="b", col="blue")
# Plot the result
plot(cv5_Tree$size, cv5_Tree$dev, type="b", col="blue")
# 10-fold Cross Validation
cv10_Tree <- cv.tree(treeFit, FUN=prune.misclass, K=10)
cv10_Tree
# Plot the result
lines(cv10_Tree$size, cv10_Tree$dev, type="b", col="red")
legend(12, 150, legend=c("5-Fold CV", "10-Fold CV"),
col=c("blue", "red"), lty=1)
legend(12, 80, legend=c("5-Fold CV", "10-Fold CV"),
col=c("blue", "red"), lty=1)
# Fitting the best prunning tree
prunedFit = prune.misclass(treeFit, best=4)
prunedFit
# Plot the Pruned Tree
plot(prunedFit)
text(prunedFit, pretty=0)
# Check the test error rate for the pruned tree
prunedPred <- predict(prunedFit, testData, type="class")
table(prunedPred,testOutcome)
mean(prunedPred != testOutcome)  # 0.1165
# Bagging
# Again, bagging is just a special case of random forest, which uses all the avaialbe
# predictors for tree splitting from the bootstrap data sets
bagTreeFit <- randomForest(homePrice~., data=trainData, mtry=13, importance=TRUE, ntree=500)
summary(bagTreeFit)
# Check the most important predictor from the data
importance(bagTreeFit)
# Plot the importance
varImpPlot(bagTreeFit)
# Check the testing error rate
bagTreePred <- predict(bagTreeFit, newdata=testData, type="class")
table(bagTreePred,testOutcome)
mean(bagTreePred != testOutcome)  # 0.11165
# Random Forest
# Now, let's see what is a good number of predictors to use for spliting the tree
rfError = rep(0,13)
for(d in 1:13){
rfTreeFit = randomForest(homePrice~., data=trainData, mtry=d, importance=TRUE, ntree=1000)
rfTreePred = predict(rfTreeFit, newdata=testData, type='class')
rfError[d] = mean(rfTreePred != testOutcome)
}
MTRY = c(1:13)
# Plot the MSE for each size of ntry
plot(MTRY, rfError, type="b", col="blue")
data.frame(MTRY, rfError)
# Use maximum 3 predictors for spliting trees from bootstrap data set
rfTreeFit = randomForest(homePrice~., data=trainData, mtry=3, importance=TRUE, ntree=500)
summary(rfTreeFit)
# Check the most important predictor from the data
importance(rfTreeFit)
# Plot the importance
varImpPlot(rfTreeFit)
# Check the testing error rate
rfTreePred <- predict(rfTreeFit, newdata=testData, type="class")
table(rfTreePred,testOutcome)
mean(rfTreePred != testOutcome)  # 0.092233
# Boosting
# Building a forest of tree with "stumps", where depth=1.
# Learning rate (shrinkage) = 0.01 and build with 5000 trees
trainData$price <- ifelse(trainData$homePrice == "High", 1, 0)
testData$price <- ifelse(testData$homePrice == "High", 1, 0)
boostTreeFit <- gbm(price~.-homePrice, data=trainData, distribution="bernoulli",
n.trees=5000, interaction.depth=1, shrinkage=0.01)
summary(boostTreeFit)
# Check the testing error rate
testOutcome2 = testData$price
boostTreePred <- predict(boostTreeFit, newdata=testData, n.trees=5000)
boostTreePred
boostTreePred <- predict(boostTreeFit, newdata=testData, n.trees=5000, type="class")
boostTreePred
boostTreePred <- ifelse(boostTreePred > 0, 1, 0)
table(boostTreePred,testOutcome2)
mean(boostTreePred != testOutcome2)  #0.92233
# Loading the required libraries for the lab
require(ISLR)
attach(Wage)
?Wage
# In this example, we are tyring to predict wages using age from the dataset
# First fit the model using the basic command for polynomial regression
fit.Poly.Basic <- lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
summary(fit.Poly.Basic)
plot(age, wage)
# We can also use the poly() function to fit the same model
fit.Poly <- lm(wage~poly(age,4, raw=TRUE),data=Wage)
summary(fit.Poly)
# Note that if we do not define raw=TRUE, the model will still have the same fitted values
# However, the coefficients for each term cannot be interpreted as usual
# In the orthogonal polynomial form, only the p-value can be interpreted
fit.Poly.Orth <- lm(wage~poly(age,4), data=Wage)
summary(fit.Poly.Orth)
# Let's create a grid of values for age for prediction
agelims <- range(age)
agelims
age.grid <- seq(agelims[1], to=agelims[2])
age.grid
preds <- predict(fit.Poly,newdata=list(age=age.grid), se=TRUE)
names(preds)
preds
se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
# Plot the data dn add the fit from the degree-4 polynomial
plot(age, wage, col="darkgray")
lines(age.grid, preds$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid, se.bands, col="red", lty=2)  #lty shows type of line lty = 2 is used for broken lines
#### Using Anova in Nested sequence of Models ####
fita <- lm(wage~education, data=Wage)
fitb <- lm(wage~education + age, data=Wage)
fitc <- lm(wage~education + poly(age,2), data=Wage)
fitd <- lm(wage~education + poly(age,3), data=Wage)
fite <- lm(wage~education + poly(age,4), data=Wage)
# The p-value indicates the model with the prior model
# The p-vlaue is significant means the prior model is not sufficient
# or the current model is better then the prior model
anova(fita,fitb,fitc,fitd,fite)
#### Logistic Regression ####
# Fitting the logistic regression model on binary response, wage,
# using wrapper I() to create binary response, over 250 (True) and below or equal 250 (False)
# on age with polynomial degree 3
fit.Logistics <- glm(I(wage>250)~poly(age,3), data=Wage, family=binomial)
summary(fit.Logistics)
# Predict the response by the age grid created earlier
preds.Logistics <- predict(fit.Logistics, newdata=list(age=age.grid), se=TRUE)
names(preds.Logistics)
# Create the confidence intervals for the prediction
se.bands <- preds.Logistics$fit+cbind(fit=0, lower=-2*preds.Logistics$se.fit, upper=2*preds.Logistics$se.fit)
se.bands[1:5,]
# Convert logistics to probabilities
prob.bands <- exp(se.bands)/(1+exp(se.bands))
prob.bands[1:5,]
# Plot the predictions with confidence intervals
matplot(age.grid, prob.bands, col="Green", lwd=c(2,1,1), lty=c(1,2,2), type="l")
range(wage)
plot(wage)
#### Fitting step functions ####
# In order to fit a step function, we need to use the cut() function
?cut
table(cut(age,4))
fit.Step <- lm(wage~cut(age,4), data=Wage)
summary(fit.Step)
table(cut(age, c(17.9, 35, 50, 65, 80.1)))
