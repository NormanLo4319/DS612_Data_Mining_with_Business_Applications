NumberofObservations # that is 400
SplitofTrainTest = 0.5 # let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Carseats[train,]
testingData  = Carseats[test,]
Testing_outcome = Sales[test] # The is the our test outcomes
# Now it is the TREE TIME :)
# You need to use training data
Tree_Model = tree(Sales~.,trainingData)
# Tree_Model = tree(Sales~CompPrice+Income+Advertising+...., trainingData)
summary(Tree_Model)
# note there is no text on the tree
plot(Tree_Model)
# Use pretty=0 only when you have categorical variables
# (ShelveLoc is a categorical variables)
text(Tree_Model, pretty=0)
# Let's check the model based on the test data
Tree_pred = predict(Tree_Model,testingData)
names(Tree_pred)
names(Tree_pred[1])
names(Tree_pred['1'])
names(Tree_pred[1])
print(Tree_pred['1'])
print(Tree_pred[1])
# Let's compute the error
mean((Tree_pred - Testing_outcome)^2)
# Five fold cross-validation
# In classification model, you will need to put in the FUN=prune.misclass parameter
cv_tree = cv.tree(Tree_Model,K=5)
# Size is the size of the tree, Dev is cross-validation Error
names(cv_tree)
cv_tree
# Our min happened at size=10, you may get a different answer based on your random seeds
plot(cv_tree$size,cv_tree$dev,type="b")
##### PRUNE THE TREE
# The objective is to reduce the level of overfitting
# May be we can do better with pruning
# you can set it to any number!
set.seed(1)
# Five fold cross-validation
# In classification model, you will need to put in the FUN=prune.misclass parameter
cv_tree = cv.tree(Tree_Model,K=5)
# Size is the size of the tree, Dev is cross-validation Error
names(cv_tree)
cv_tree
# Our min happened at size=10, you may get a different answer based on your random seeds
plot(cv_tree$size,cv_tree$dev,type="b")
# Now let's prune our tree - that is the gardening time!
pruned_Model = prune.tree(Tree_Model, best=9) #We pruned our model based on best size we found
# in cross-validation - remember it was 10 (you might have found another number)
plot(pruned_Model)
text(pruned_Model,pretty=0)
# Let's compute the error
Tree_pred_new = predict(pruned_Model,testingData)
mean((Tree_pred_new - Testing_outcome)^2) #error decreased to 4.918134
# Let's compute the error
mean((Tree_pred - Testing_outcome)^2)
# It is from 0 to 16.27
# so, let's split them in half and call high sales for
# Sales more than 8
High = ifelse(Sales >= 8, "Yes", "No")
Carseats = data.frame(Carseats, High)
dim(Carseats)
# Let's get rid of Sales data - Since we already have High
Carseats = Carseats[,-1]
head(Carseats)
#you can set.seed to any number
set.seed(2)
NumberofObservations = nrow(Carseats)
NumberofObservations # that is 400
SplitofTrainTest = 0.5 # let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Carseats[train,]
testingData  = Carseats[test,]
Testing_outcome = High[test] # The is the our test outcomes
# Now it is the TREE TIME :)
# You need to use training data
Tree_Model = tree(High~.,trainingData)
plot(Tree_Model) # note there is no text on the tree
text(Tree_Model, pretty = 0) # Use pretty = 0 only when you have categorical variables
#Let's check the model based on the test data
Tree_pred = predict(Tree_Model,testingData,type="class") #Since our predictions are on categorical variables we used type = "class"
# Let's compute the error
mean(Tree_pred != Testing_outcome) #0.23
# Since we dealt with classification we neede ot set FUN to prune.misclass
cv_tree = cv.tree(Tree_Model, FUN=prune.misclass)
# Size is the size of the tree, Dev is cross-validation error rate
names(cv_tree)
plot(cv_tree$size,cv_tree$dev,type="b")
# Now let's prune our tree - that is the gardening time!
# We pruned our model based on best size we found
pruned_Model = prune.misclass(Tree_Model, best=9)
# in cross-validation - remember it was 9 (you might have found another number)
plot(pruned_Model)
text(pruned_Model)
Tree_pred_new = predict(pruned_Model,testingData,type="class")
mean(Tree_pred_new != Testing_outcome)
# Remove or clear all memories in the global environment
rm(list=ls(all=TRUE))
# Remove or clear all memories in the global environment
rm(list=ls(all=TRUE))
# book's Libraray
library(ISLR)
# This libraray is essential to fit decision trees
library(tree)
# The dataset is from ISLR Library
attach(Carseats)
# let's explore the dataset
head(Carseats)
dim(Carseats)
# First let's look up the range for Sales
range(Sales)
# It is from 0 to 16.27
# so, let's split them in half and call high sales for
# Sales more than 8
High = ifelse(Sales >= 8, "Yes", "No")
Carseats = data.frame(Carseats, High)
dim(Carseats)
# Let's get rid of Sales data - Since we already have High
Carseats = Carseats[,-1]
head(Carseats)
#you can set.seed to any number
set.seed(2)
NumberofObservations = nrow(Carseats)
NumberofObservations # that is 400
SplitofTrainTest = 0.5 # let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Carseats[train,]
testingData  = Carseats[test,]
High[test]
Testing_outcome = High[test] # The is the our test outcomes
# Now it is the TREE TIME :)
# You need to use training data
Tree_Model = tree(High~., trainingData)
summary(Tree_Model)
plot(Tree_Model) # note there is no text on the tree
text(Tree_Model, pretty=0) # Use pretty = 0 only when you have categorical variables
#Let's check the model based on the test data
Tree_pred = predict(Tree_Model,testingData,type="class") #Since our predictions are on categorical variables we used type = "class"
# Let's compute the error
mean(Tree_pred != Testing_outcome) #0.23
# May be we can do better with pruning
# again, you can set it to any number!
set.seed(3)
# Since we dealt with classification we neede ot set FUN to prune.misclass
cv_tree = cv.tree(Tree_Model, FUN=prune.misclass)
# Size is the size of the tree, Dev is cross-validation error rate
names(cv_tree)
cv_tree
plot(cv_tree$size, cv_tree$dev, type="b")
# May be we can do better with pruning
# again, you can set it to any number!
set.seed(2)
# Since we dealt with classification we neede ot set FUN to prune.misclass
cv_tree = cv.tree(Tree_Model, FUN=prune.misclass)
# Size is the size of the tree, Dev is cross-validation error rate
names(cv_tree)
cv_tree
plot(cv_tree$size, cv_tree$dev, type="b")
# May be we can do better with pruning
# again, you can set it to any number!
set.seed(1)
# Since we dealt with classification we neede ot set FUN to prune.misclass
cv_tree = cv.tree(Tree_Model, FUN=prune.misclass)
# Size is the size of the tree, Dev is cross-validation error rate
names(cv_tree)
cv_tree
plot(cv_tree$size, cv_tree$dev, type="b")
# Now let's prune our tree - that is the gardening time!
# We pruned our model based on best size we found
pruned_Model = prune.misclass(Tree_Model, best=9)
# in cross-validation - remember it was 9 (you might have found another number)
plot(pruned_Model)
text(pruned_Model)
# in cross-validation - remember it was 9 (you might have found another number)
plot(pruned_Model)
text(pruned_Model, pretty=0)
Tree_pred_new = predict(pruned_Model, testingData, type="class")
mean(Tree_pred_new != Testing_outcome)
# Let's compute the error
mean(Tree_pred != Testing_outcome) #0.23
# Import the libraries for analysis
library(randomForest)
library(ISLR)
library(tree)
library(MASS)
set.seed(1)
NumberofObservations = dim(Boston)[1]
SplitofTrainTest = 0.5 #let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Boston[train,]
testingData  = Boston[test,]
Testing_outcome = Boston$medv[test]
?randomForest
# Bagging is simply a special case of random forest with m = p (All predictors are used to grow trees)
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE)
# mtry is the nymber of variables randomly sampled as candidates at each slpit.
# The defult is sqrt(p) in classification and p/3 in regression
# The defult number of trees is 500
bag.boston
# Bagging is simply a special case of random forest with m = p (All predictors are used to grow trees)
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE)
# Using the importance() function to check the importance of each variable
importance(bag.boston)
# We can plot these importance measures with the varImpPlot() function
varImpPlot(bag.boston)
# Make prediction with the trained model by passing in the testing dataset
predict.bag = predict(bag.boston,newdata = testingData)
# Plot the prediction and testing outcome
plot(predict.bag,Testing_outcome)
abline(0,1)
# Calculate the MSE for the bagging model
MSE.bag = mean((predict.bag - Testing_outcome )^2)
MSE.bag
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=25)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.25Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.25Trees #Our error increased
# Using a for loop to run different size of ntry
MSE.Rf=rep(0,13)
for(d in 1:13){
rf.boston = randomForest(medv~., data=trainingData, mtry=d, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[d] = mean((predict.rf- Testing_outcome )^2)
}
MTRY = c(1:13)
# Plot the MSE for each size of ntry
plot(MTRY,MSE.Rf,type="b",col="red")
min(MSE.Rf)
# mtry=4 created the minimum error - if you repeat this over and over you may get another miminizers such as 4 or 5
rf.boston = randomForest(medv~., data=trainingData, mtry=4, importance=TRUE)
# Getting the prediction from the best model
predict.rf = predict(rf.boston, newdata=testingData)
# Calculate the MSE from the best model
MSE.Rf = mean((predict.rf- Testing_outcome )^2)
MSE.Rf
# Check the importance measures
importance(rf.boston)
# Plot the importance measures
varImpPlot(rf.boston)
plot(rf.boston)
library(gbm)
set.seed(1)
?gbm
# If you are running regression problems then use distribution = "gaussian". If you are working on
# binary classfiication problems, then use distribution = "bernoulli"
# The default value of Lambda is 0.001
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4)
boost.boston
summary(boost.boston)
summary(Boston$medv)
# Parial dependence plots for rm and lstat
# These plots illustrate the marginal effect of the selected variables on the response (medv) after
# integerating out the other variables. as we expect medv is increasing with rm and decreasing with lstat
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Get the prediction from the boosting model
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
# Calculate the MSE of the boosting model
mean((predict.boost-Testing_outcome )^2)
# Let's Change Lambda
Lambda = c(.00001,0.0001,0.001,.01,0.1,.15,.2)
Counter = 1
MSE.Boost = rep(0,7)
# Using a for loop to check different values for Lamda
for(d in Lambda){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4, shrinkage=d)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
MSE.Boost[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# The miminum happened at Lambda = 0.01
plot(Lambda,MSE.Boost,type="b",col="red")
# 10.31 much less than random forest
min(MSE.Boost)
# Now let's fix Lambda and change size of the tree
TreeSize = c(50,100,200,400,500,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000)
Counter = 1
MSE.Boost.Tree = rep(0,15)
# Create a for loop to check different tree size with the best Lamda selected (shrinkage=0.01)
for(d in TreeSize){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=d, interaction.depth=4, shrinkage=0.01)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=d)
MSE.Boost.Tree[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# The miminum happened at Lambda = 0.01
plot(TreeSize,MSE.Boost.Tree,type="b",col="red")
# it seems like 5000 was a very good choice
min(MSE.Boost.Tree)
MSE.Boost.Tree
# Import the libraries for analysis
library(randomForest)
library(ISLR)
library(tree)
library(MASS)
set.seed(1)
NumberofObservations = dim(Boston)[1]
SplitofTrainTest = 0.5 #let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Boston[train,]
testingData  = Boston[test,]
Testing_outcome = Boston$medv[test]
?randomForest
# Bagging is simply a special case of random forest with m = p (All predictors are used to grow trees)
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE)
# mtry is the nymber of variables randomly sampled as candidates at each slpit.
# The defult is sqrt(p) in classification and p/3 in regression
# The defult number of trees is 500
bag.boston
names(bag.boston)
summary(bag.boston)
# Using the importance() function to check the importance of each variable
importance(bag.boston)
# We can plot these importance measures with the varImpPlot() function
varImpPlot(bag.boston)
# Make prediction with the trained model by passing in the testing dataset
predict.bag = predict(bag.boston, newdata=testingData)
# Plot the prediction and testing outcome
plot(predict.bag,Testing_outcome)
abline(0,1)
# Calculate the MSE for the bagging model
MSE.bag = mean((predict.bag - Testing_outcome )^2)
MSE.bag
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=25)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.25Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.25Trees #Our error increased
# Using a for loop to run different size of mtry (number of predictors to be consider for each split of the tree)
MSE.Rf=rep(0,13)
for(d in 1:13){
rf.boston = randomForest(medv~., data=trainingData, mtry=d, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[d] = mean((predict.rf- Testing_outcome )^2)
}
MTRY = c(1:13)
# Plot the MSE for each size of ntry
plot(MTRY,MSE.Rf,type="b",col="red")
min(MSE.Rf)
# mtry=4 created the minimum error - if you repeat this over and over you may get another miminizers such as 4 or 5
rf.boston = randomForest(medv~., data=trainingData, mtry=4, importance=TRUE)
TreeSize = c(50,100,200,400,500,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000)
Counter = 1
MSE.Boost.Tree = rep(0,15)
for(i in TreeSize){
rf.boston = randomForest(medv~., data=trainingData, mtry=i, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[Counter] = mean((predict.rf- Testing_outcome )^2)
Counter += 1
}
for(i in TreeSize){
rf.boston = randomForest(medv~., data=trainingData, mtry=i, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[Counter] = mean((predict.rf- Testing_outcome )^2)
Counter += 1
}
for(i in TreeSize){
rf.boston = randomForest(medv~., data=trainingData, mtry=i, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[Counter] = mean((predict.rf- Testing_outcome )^2)
Counter = Counter + 1
}
# Plot the MSE for each size of ntry
plot(TreeSize,MSE.Rf,type="b",col="red")
min(MSE.Rf)
?randomForest
for(i in TreeSize){
bag.boston = randomForest(medv~., data=trainingData, mtry=13, ntree=i, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[Counter] = mean((predict.rf- Testing_outcome )^2)
Counter = Counter + 1
}
# Plot the MSE for each size of ntry
plot(TreeSize,MSE.Rf,type="b",col="red")
TreeSize = c(50,100,200,400,500,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000)
Counter = 1
MSE.Bag.Tree = rep(0,15)
for(i in TreeSize){
bag.boston = randomForest(medv~., data=trainingData, mtry=13, ntree=i, importance=TRUE)
predict.big = predict(bag.boston, newdata=testingData)
MSE.Bag.Tree[Counter] = mean((predict.bag- Testing_outcome )^2)
Counter = Counter + 1
}
# Plot the MSE for each size of ntry
plot(TreeSize,MSE.Big.Tree,type="b",col="red")
# Plot the MSE for each size of ntry
plot(TreeSize,MSE.Bag.Tree,type="b",col="red")
# Loading the required libraries for the lab
require(ISLR)
# In this example, we are tyring to predict wages using age from the dataset
# First fit the model using the basic command for polynomial regression
fit.Poly.Basic = lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
summary(fit.Poly.Basic)
# We can also use the poly() function to fit the same model
fit.Poly =lm(wage~poly(age,4,raw=TRUE),data=Wage)
summary(fit.Poly)
# Note that if we do not define raw=TRUE, the model will still have the same fitted values
# However, the coefficients for each term cannot be interpreted as usual
# In the orthogonal polynomial form, only the p-value can be interpreted
fit.Poly.Orth =lm(wage~poly(age,4),data=Wage)
summary(fit.Poly.Orth)
# Let's create a grid of valuesfor age for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
preds = predict(fit.Poly,newdata = list(age=age.grid),se = TRUE)
names(preds)
se.bands = cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
attach(Wage)
# Let's create a grid of valuesfor age for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
preds = predict(fit.Poly,newdata = list(age=age.grid),se = TRUE)
names(preds)
se.bands = cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
se.bands
# Plot the data dn add the fit from the degree-4 polynomial
plot(age,wage,col="darkgray")
lines(age.grid,preds$fit,lwd=2,col="blue") #lwd is linewidth
matlines(age.grid,se.bands,col="blue",lty=2)  #lty shows type of line lty = 2 is used for broken lines
#### Using Anova in Nested sequence of Models ####
fita = lm(wage~education,data=Wage)
fitb = lm(wage~education + age,data=Wage)
fitc = lm(wage~education + poly(age,2),data=Wage)
fitd = lm(wage~education + poly(age,3),data=Wage)
fite = lm(wage~education + poly(age,4),data=Wage)
# The p-value indicates the model with the prior model
# The p-vlaue is significant means the prior model is not sufficient
# or the current model is better then the prior model
anova(fita,fitb,fitc,fitd,fite)
#### Logistic Regression ####
fit.Logistics = glm(I(wage>250)~poly(age,3), data=Wage, family=binomial)
summary(fit.Logistics)
preds.Logistics = predict(fit.Logistics, newdata=list(age=age.grid), se=TRUE)
names(preds.Logistics)
se.bands = preds.Logistics$fit+cbind(fit=0, lower=-2*preds.Logistics$se.fit, upper=2*preds.Logistics$se.fit)
se.bands[1:5,]
# Convert logistics to probabilities
prob.bands = exp(se.bands)/(1+exp(se.bands))
prob.bands[1:5,]
matplot(age.grid,prob.bands,col="blue",lwd=c(2,1,1),lty=c(1,2,2),type="l")
age.grid
#### Splines ####
library(splines)
library(ISLR)
# The bs() function generates the entire matrix of basis functions for splines
# with the specified set of knots. By default the cubic splines are produced.
attach(Wage)
# We speified knots at ages 25, 40, and 60, which produces a spline with 6 basis functions
fit.splines = lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
summary(fit.splines)
# Create a age grid for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
# Predict wage based on the age grid
pred.Splines = predict(fit.splines, newdata=list(age=age.grid), se=TRUE)
names(pred.Splines)
# Plot the prediction and confidence intervals
plot(age,wage,col="gray")
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit,preds$fit-2*pred.Splines$se.fit)
lines(age.grid,pred.Splines$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid,se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
## We can change degrees of freedom and use Natural Smoothing with the ns() function
fit.splines2 = lm(wage~ns(age,df=4),data=Wage)
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
#### Smoothing Splines ####
plot(age,wage,col="gray")
# Fitting the smooth spline by using the smooth.spline() function
# We use 16 degree of freedom to determines the value of lamda
fit.SS = smooth.spline(age, wage, df=16)
# We then select the smoothness level by cross-validation with cv=TRUE
fit.SS2 = smooth.spline(age, wage, cv=TRUE)
names(fit.SS2)
fit.SS2$df
lines(fit.SS, col="red", lwd=2)
lines(fit.SS2, col="blue", lwd=2)
#### Local Regression Lines #### loess()
# span = the neighborhood it should consider for analysis
fit.LRL = loess(wage~age, span=0.2, data=Wage)
Predict.LRL = predict(fit.LRL, newdata=data.frame(age=age.grid)) #we shall use data.frame instead of list
fit.LRL2 = loess(wage~age, span=0.5, data=Wage)
Predict.LRL2 = predict(fit.LRL2, newdata=data.frame(age=age.grid))
plot(age, wage, col="gray")
lines(age.grid,Predict.LRL, col="blue", lwd=2)
lines(age.grid,Predict.LRL2, col="red", lwd=2)
### GAMS ####
install.packages("gam")
library(gam)
#s() is smoothing spline in GAMs libarary
gam1 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
plot(gam1, se=TRUE, col="blue")  #not an impressive representation"
par(mfrow=c(1,3)) #Show 3 outcomes 1 for each variable  year, age, education
plot.gam(gam1,se=TRUE, col="red")
summary(gam1)
### ANOVA IN GAM ###
gam.V1 = gam(wage~s(age,5)+education, data=Wage)
gam.V2 = gam(wage~year+s(age,5)+education, data=Wage)
gam.V3 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
anova(gam.V1, gam.V2, gam.V3, test="F")
PredictGam = predict(gam.V2, newdata=Wage)
PredictGam
##### Adding Local regression fits by lo() models
gam.lo = gam(wage~s(year,df=4)+lo(age,span=0.5)+education, data=Wage)
##### Finally that is the time for logistic regression in GAM #####
## You can use I() and don't forget to set family to binomial
gam.lr = gam(I(wage>250)~year+s(age,df=5)+education, family=binomial)
plot(gam.lr, se=TRUE, col="green")
