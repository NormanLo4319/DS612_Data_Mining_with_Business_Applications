test = -train
trainData = Carseats[train,]
testData  = Carseats[test,]
testOutcome = Carseats$Sales[test]
tree.fit <- tree(Sales~., data=trainData)
summary(tree.fit)
tree.fit <- tree(Sales~ShelveLoc+Price+Age+Income+CompPrice+Advertising, data=trainData)
summary(tree.fit)
logitFit <- glm(Sales~ShelveLoc+Price+Age+Income+CompPrice+Advertising, data=trainData, family=binomial)
logitFit <- glm(Sales~ShelveLoc+Price+Age+Income+CompPrice+Advertising, data=trainData, family=binomial)
logitFit <- glm(High~ShelveLoc+Price+Age+Income+CompPrice+Advertising, data=trainData, family=binomial)
logitFit <- glm(High~ShelveLoc+Price+Age+Income+CompPrice+Advertising-Sales, data=trainData, family=binomial)
#  Carseat Problem
rm(list=ls(all=TRUE))
High <- ifelse(Carseats$Sales <= 7, "No", "Yes")
set.seed(1)
n = nrow(Carseats)
s = 0.7
train = sample(1:n, n*s)
test = -train
trainData = Carseats[train,]
testData  = Carseats[test,]
testOutcome = Carseats$High[test]
testOutcome = High[test]
logitFit <- glm(High~ShelveLoc+Price+Age+Income+CompPrice+Advertising-Sales, data=trainData, family=binomial)
#  Carseat Problem
rm(list=ls(all=TRUE))
High <- ifelse(Carseats$Sales <= 7, "No", "Yes")
Carseats = data.frame(Carseats, High)
set.seed(1)
n = nrow(Carseats)
s = 0.7
train = sample(1:n, n*s)
test = -train
trainData = Carseats[train,]
testData  = Carseats[test,]
testOutcome = High[test]
logitFit <- glm(High~ShelveLoc+Price+Age+Income+CompPrice+Advertising-Sales, data=trainData, family=binomial)
Summary(logitFit)
Summary(logitFit)
summary(logitFit)
treeFit <- tree(High~ShelveLoc+Price+Age+Income+CompPrice+Advertising-Sales, data=trainData)
summary(tree.fit)
summary(treeFit)
carseats = data.frame(High, Shelveloc, Price, Age, Income, CompPrice, Advertising)
carseats = data.frame(High, ShelveLoc, Price, Age, Income, CompPrice, Advertising)
#  Carseat Problem
rm(list=ls(all=TRUE))
High <- ifelse(Carseats$Sales <= 7, "No", "Yes")
carseats = data.frame(High, ShelveLoc, Price, Age, Income, CompPrice, Advertising)
set.seed(1)
n = nrow(carseats)
s = 0.7
train = sample(1:n, n*s)
test = -train
trainData = carseats[train,]
testData  = carseats[test,]
testOutcome = High[test]
logitFit <- glm(High~., data=trainData, family=binomial)
summary(logitFit)
treeFit <- tree(High~., data=trainData)
summary(treeFit)
ldaFit <- lda(High~., data=trainData)
summary(ldaFit)
ldaFit
cvTree <- cv.tree(treeFit, Fun=prune.misclass)
cvTree <- cv.tree(treeFit, FUN=prune.misclass)
cvTree
pruneTree <- prune.misclass(treeFit, best=8)
summary(pruneTree)
ladFit
qdaFit <- qda(High~. data=trainData)
qdaFit
qdaFit <- qda(High~. data=trainData)
qdaFit <- qda(High~., data=trainData)
qdaFit
rfMSE=rep(0,6)
for(d in 1:6){
rfFit = randomForest(High~., data=trainData, mtry=6, importance=TRUE)
rfPred = predict(rfFit, newdata=testData)
rfMSE[d] = mean((rfPred - testOutcome )^2)
}
rfMSE=rep(0,6)
for(d in 1:6){
rfFit = randomForest(High~., data=trainData, mtry=6, importance=TRUE)
rfPred = predict(rfFit, newdata=testData)
rfMSE[d] = mean(rfPred != testOutcome)
}
MTRY = c(1:13)
MTRY = c(1:6)
plot(MTRY, rfMSE)
bagFit <- randomForest(High~., data=trainData, mtry=6, importance=TRUE)
bagPred <- predict(rfFit, newdata=testData)
bagPred
bagER <- mean(bagPred != testOutcome)
bagER
rfFit = randomForest(High~., data=trainData, mtry=2, importance=TRUE)
rfPred = predict(rfFit, newdata=testData)
rfER[d] = mean(rfPred != testOutcome)
rfER = mean(rfPred != testOutcome)
rfER
pruneTreePred <- predict(pruneTree, newdata=testData, type="class")
pruneTreeER <- mean(pruneTreePred != testOucome)
pruneTreeER
pruneTreeER <- mean(pruneTreePred != testOutcome)
pruneTreeER
bagER
rfER
qdaPred <- predict(pdaFit, data=trainData)$class
qdaPred <- predict(qdaFit, data=trainData)$class
qdaER <- mean(qdaPred != qdaPred)
qdaER <- mean(qdaPred != testOutcome)
qdaER
ldaPred <- predict(ldaFit, data=trainData)$class
ldaER <- mean(ldaPred != testOutcome)
ldaPred <- predict(ldaFit, data=testData)$class
ldaER <- mean(ldaPred != testOutcome)
qdaPred <- predict(qdaFit, data=testData)$class
qdaER <- mean(qdaPred != testOutcome)
qdaER
ldapred
ldaPred
ldaPred <- predict(ldaFit, data=testData)$class
ldaPred
qdaPred
pruneTreePred
qdaPred <- predict(qdaFit, newdata=testData)$class
qdaPred
ldaPred <- predict(ldaFit, newdata=testData)$class
ldaER <- mean(ldaPred != testOutcome)
ldaER
qdaER
qdaER <- mean(qdaPred != testOutcome)
qdaER
logitPred <- predict(logitFit, newdata=testData, type="response")
logitPred
logitProbs <- predict(logitFit, newdata=testData, type="response")
logitPred <- rep("No", 120)
logitPred[ligitProbs > 0.5] <- "Yes"
logitPred[ligitProbs > 0.5] = "Yes"
logitPred[logitProbs > 0.5] = "Yes"
logitER <- mean(logitPred != testOutcome)
logitER
labs(y="Testing Error Rate", x="Models");
models <- c("Logit", "LDA", "QDA", "PruneTree", "BagForest", "RandomForest")
errors <- c(logitER, ldaER, qdaER, pruneTreeER, bagER, rfER)
ggplot(results, aes(as.factor(models), errors)) +
geom_point() +
geom_line()
results <- data.frame(models, errors)
ggplot(results, aes(as.factor(models), errors)) +
geom_point() +
geom_line()
ggplot(results, aes(as.factor(models), errors)) +
geom_point() +
geom_line() +
labs(y="Testing Error Rate", x="Models");
ggplot(results, aes(as.factor(models), errors)) + geom_point() + geom_line() +
labs(y="Testing Error Rate", x="Models");
ggplot(results, aes(as.factor(models), errors)) + geom_point() + geom_line(aes(as.factor(models), errors)) +
labs(y="Testing Error Rate", x="Models");
ggplot(results, aes(as.factor(models), errors, group=1)) + geom_point() + geom_line() +
labs(y="Testing Error Rate", x="Models");
validation <- c("VS", "VS", "VS", "VS", "VS", "VS")
results <- data.frame(models, errors, validation)
logitFit <- glm(High~., data=carseats, family=binomial)
logitloocv <- cv.glm(carseats, logitFit)$delta[1]
logitloocv
ldaFit <- lda(High~., data=carseats)
ldaloocv <- cv.glm(carseats, ldaFit)$delta[1]
library(MASS)
DA.valid(ldaFit)
?DA.valid
??DA.valid
ldaFit <- lda(High~., data=carseats, cv=TRUE)
ldaFit <- lda(High~., data=carseats, CV=TRUE)
#### Splines ####
library(splines)
library(ISLR)
# The bs() function generates the entire matrix of basis functions for splines
# with the specified set of knots. By default the cubic splines are produced.
attach(Wage)
# We speified knots at ages 25, 40, and 60, which produces a spline with 6 basis functions
fit.splines = lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
summary(fit.splines)
# Create a age grid for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
# Predict wage based on the age grid
pred.Splines = predict(fit.splines, newdata=list(age=age.grid), se=TRUE)
names(pred.Splines)
# Plot the prediction and confidence intervals
plot(age,wage,col="gray")
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit,preds$fit-2*pred.Splines$se.fit)
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit, pred.Splines$fit-2*pred.Splines$se.fit)
lines(age.grid,pred.Splines$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid,se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
## We can change degrees of freedom and use Natural Smoothing with the ns() function
fit.splines2 = lm(wage~ns(age,df=4),data=Wage)
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
#### Smoothing Splines ####
plot(age,wage,col="gray")
# Fitting the smooth spline by using the smooth.spline() function
# We use 16 degree of freedom to determines the value of lamda
fit.SS = smooth.spline(age, wage, df=16)
# We then select the smoothness level by cross-validation with cv=TRUE
fit.SS2 = smooth.spline(age, wage, cv=TRUE)
names(fit.SS2)
# The CV selected degree of freedom = 6.794596
fit.SS2$df
# Plot the two fitted model for comparison
lines(fit.SS, col="red", lwd=2)
lines(fit.SS2, col="blue", lwd=2)
#### Local Regression Lines #### loess()
# span = the neighborhood it should consider for analysis
fit.LRL = loess(wage~age, span=0.2, data=Wage)
Predict.LRL = predict(fit.LRL, newdata=data.frame(age=age.grid)) #we shall use data.frame instead of list
fit.LRL2 = loess(wage~age, span=0.5, data=Wage)
Predict.LRL2 = predict(fit.LRL2, newdata=data.frame(age=age.grid))
plot(age, wage, col="gray")
lines(age.grid,Predict.LRL, col="blue", lwd=2)
lines(age.grid,Predict.LRL2, col="red", lwd=2)
### GAMS ####
install.packages("gam")
library(gam)
#s() is smoothing spline in GAMs libarary
gam1 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
plot(gam1, se=TRUE, col="blue")  #not an impressive representation"
par(mfrow=c(1,3)) #Show 3 outcomes 1 for each variable  year, age, education
plot.gam(gam1,se=TRUE, col="red")
summary(gam1)
### ANOVA IN GAM ###
gam.V1 = gam(wage~s(age,5)+education, data=Wage)
gam.V2 = gam(wage~year+s(age,5)+education, data=Wage)
gam.V3 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
anova(gam.V1, gam.V2, gam.V3, test="F")
# Make predictions with the best model
PredictGam = predict(gam.V2, newdata=Wage)
PredictGam
##### Adding Local regression fits by lo() models
gam.lo = gam(wage~s(year,df=4)+lo(age,span=0.5)+education, data=Wage)
##### Finally that is the time for logistic regression in GAM #####
## You can use I() and don't forget to set family to binomial
gam.lr = gam(I(wage>250)~year+s(age,df=5)+education, family=binomial)
plot(gam.lr, se=TRUE, col="green")
summary(fit.splines)
# Plot the prediction and confidence intervals
plot(age,wage,col="gray")
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit, pred.Splines$fit-2*pred.Splines$se.fit)
lines(age.grid,pred.Splines$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid,se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
## We can change degrees of freedom and use Natural Smoothing with the ns() function
fit.splines2 = lm(wage~ns(age,df=4),data=Wage)
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
## We can change degrees of freedom and use Natural Smoothing with the ns() function
fit.splines2 = lm(wage~ns(age,df=3),data=Wage)
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
## We can change degrees of freedom and use Natural Smoothing with the ns() function
fit.splines2 = lm(wage~ns(age,df=6),data=Wage)
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
## We can change degrees of freedom and use Natural Smoothing with the ns() function
fit.splines2 = lm(wage~ns(age,df=4),data=Wage)
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
# We speified knots at ages 25, 40, and 60, which produces a spline with 6 basis functions
# The bs() function, by default, genearte a cubic splines model
fit.splines = lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
summary(fit.splines)
# Create an age grid for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
# Predict wage based on the age grid
pred.Splines = predict(fit.splines, newdata=list(age=age.grid), se=TRUE)
names(pred.Splines)
# Plot the prediction and confidence intervals
plot(age,wage,col="gray")
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit, pred.Splines$fit-2*pred.Splines$se.fit)
lines(age.grid,pred.Splines$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid,se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
rm(list=ls(all=TRUE))
#### Splines ####
library(splines)
library(ISLR)
# The bs() function generates the entire matrix of basis functions for splines
# with the specified set of knots. By default the cubic splines are produced.
attach(Wage)
# We speified knots at ages 25, 40, and 60, which produces a spline with 6 basis functions
# The bs() function, by default, genearte a cubic splines model
fit.splines = lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
summary(fit.splines)
# Create an age grid for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
# Predict wage based on the age grid
pred.Splines = predict(fit.splines, newdata=list(age=age.grid), se=TRUE)
# Plot the prediction and confidence intervals
plot(age,wage,col="gray")
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit, pred.Splines$fit-2*pred.Splines$se.fit)
lines(age.grid,pred.Splines$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid,se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
## We can change degrees of freedom and use Natural Spline with the ns() function
# In this example we are using df=4 to define 3 internal knots for the model.
# By default, ns() function will pick 25th, 50th, and 75th percetiles of age,
# which are 33.8, 42, and 51 as the internal knots.
fit.splines2 = lm(wage~ns(age,df=4),data=Wage)
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
se.bands2 = cbind(pred.Splines2$fit+2*pred.Splines2$se.fit, pred.Splines2$fit-2*pred.Splines2$se.fit)
matlines(age.grid,se.bands2, col="red", lty=2)
summary(fit.splines2)
#### Smoothing Splines ####
plot(age,wage,col="gray")
# Fitting the smooth spline by using the smooth.spline() function
# We use 16 degree of freedom to determines the value of lamda
fit.SS = smooth.spline(age, wage, df=16)
# We then select the smoothness level by cross-validation with cv=TRUE
fit.SS2 = smooth.spline(age, wage, cv=TRUE)
# The CV selected degree of freedom = 6.794596
fit.SS2$df
# Plot the two fitted model for comparison
lines(fit.SS, col="red", lwd=2)
lines(fit.SS2, col="blue", lwd=2)
#### Local Regression Lines #### loess()
# span = the neighborhood it should consider for analysis
fit.LRL = loess(wage~age, span=0.2, data=Wage)
Predict.LRL = predict(fit.LRL, newdata=data.frame(age=age.grid)) #we shall use data.frame instead of list
fit.LRL2 = loess(wage~age, span=0.5, data=Wage)
Predict.LRL2 = predict(fit.LRL2, newdata=data.frame(age=age.grid))
plot(age, wage, col="gray")
lines(age.grid,Predict.LRL, col="blue", lwd=2)
lines(age.grid,Predict.LRL2, col="red", lwd=2)
library(gam)
#s() is smoothing spline in GAMs libarary
gam1 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
plot(gam1, se=TRUE, col="blue")  #not an impressive representation"
par(mfrow=c(1,3)) #Show 3 outcomes 1 for each variable  year, age, education
plot.gam(gam1,se=TRUE, col="red")
summary(gam1)
### ANOVA IN GAM ###
gam.V1 = gam(wage~s(age,5)+education, data=Wage)
gam.V2 = gam(wage~year+s(age,5)+education, data=Wage)
gam.V3 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
anova(gam.V1, gam.V2, gam.V3, test="F")
# Make predictions with the best model
PredictGam = predict(gam.V2, newdata=Wage)
PredictGam
##### Adding Local regression fits by lo() models
gam.lo = gam(wage~s(year,df=4)+lo(age,span=0.5)+education, data=Wage)
##### Finally that is the time for logistic regression in GAM #####
## You can use I() and don't forget to set family to binomial
gam.lr = gam(I(wage>250)~year+s(age,df=5)+education, family=binomial)
plot(gam.lr, se=TRUE, col="green")
# The convenience for fitting a GAM model is that we can mix with different non-linear models
# s() is smoothing spline in GAMs library
# ns() is natural spline in GAMs library
# lo() is local regression in GAMs libarary
gam1 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
# Plot the prediction
plot(gam1, se=TRUE, col="blue")  #not an impressive representation"
par(mfrow=c(1,3)) #Show 3 outcomes 1 for each variable  year, age, education
plot(gam.m3, se=TRUE, col='blue')
# The convenience for fitting a GAM model is that we can mix with different non-linear models
# s() is smoothing spline in GAMs library
# ns() is natural spline in GAMs library
# lo() is local regression in GAMs libarary
gam1 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
# Plot the prediction
plot(gam1, se=TRUE, col="blue")  #not an impressive representation"
par(mfrow=c(1,3)) #Show 3 outcomes 1 for each variable  year, age, education
# Plot the prediction
plot(gam1, se=TRUE, col="blue")  #not an impressive representation"
par(mfrow=c(1,3)) #Show 3 outcomes 1 for each variable  year, age, education
# Plot the prediction
plot(gam1, se=TRUE, col="blue")  #not an impressive representation"
summary(gam1)
rm(list=ls(all=TRUE))
#### Splines ####
library(splines)
library(ISLR)
# The bs() function generates the entire matrix of basis functions for splines
# with the specified set of knots. By default the cubic splines are produced.
attach(Wage)
?Wage
# We speified knots at ages 25, 40, and 60, which produces a spline with 6 basis functions
# The bs() function, by default, genearte a cubic splines model
fit.splines = lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
summary(fit.splines)
# Create an age grid for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
# Predict wage based on the age grid
pred.Splines = predict(fit.splines, newdata=list(age=age.grid), se=TRUE)
names(pred.Splines)
# Plot the prediction and confidence intervals
plot(age,wage,col="gray")
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit, pred.Splines$fit-2*pred.Splines$se.fit)
lines(age.grid,pred.Splines$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid,se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
## We can change degrees of freedom and use Natural Spline with the ns() function
# In this example we are using df=4 to define 3 internal knots for the model.
# By default, ns() function will pick 25th, 50th, and 75th percetiles of age,
# which are 33.8, 42, and 51 as the internal knots.
fit.splines2 = lm(wage~ns(age,df=4),data=Wage)
summary(fit.splines2)
# Predict wage based on the age grid
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
# Plot the prediction and confidence intervals
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
se.bands2 = cbind(pred.Splines2$fit+2*pred.Splines2$se.fit, pred.Splines2$fit-2*pred.Splines2$se.fit)
matlines(age.grid,se.bands2, col="red", lty=2)
# Fitting the smooth spline by using the smooth.spline() function
# We use 16 degree of freedom to determines the value of lamda
fit.SS = smooth.spline(age, wage, df=16)
# We then select the smoothness level by cross-validation with cv=TRUE
fit.SS2 = smooth.spline(age, wage, cv=TRUE)
names(fit.SS2)
# The CV selected degree of freedom = 6.794596
fit.SS2$df
# Plot the two fitted model for comparison
plot(age,wage,col="gray")
source('C:/Users/lokma/Desktop/Teaching/DS612/DS612_Data_Mining_with_Business_Applications/DS612_R-Code_Lecture_7.2.R', echo=TRUE)
lines(fit.SS, col="red", lwd=2)
# Plot the two fitted model for comparison
plot(age,wage,col="gray")
rm(list=ls(all=TRUE))
#### Splines ####
library(splines)
library(ISLR)
# The bs() function generates the entire matrix of basis functions for splines
# with the specified set of knots. By default the cubic splines are produced.
attach(Wage)
?Wage
# We speified knots at ages 25, 40, and 60, which produces a spline with 6 basis functions
# The bs() function, by default, genearte a cubic splines model
fit.splines = lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
summary(fit.splines)
# Create an age grid for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
# Predict wage based on the age grid
pred.Splines = predict(fit.splines, newdata=list(age=age.grid), se=TRUE)
names(pred.Splines)
# Plot the prediction and confidence intervals
plot(age,wage,col="gray")
se.bands = cbind(pred.Splines$fit+2*pred.Splines$se.fit, pred.Splines$fit-2*pred.Splines$se.fit)
lines(age.grid,pred.Splines$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid,se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
## We can change degrees of freedom and use Natural Spline with the ns() function
# In this example we are using df=4 to define 3 internal knots for the model.
# By default, ns() function will pick 25th, 50th, and 75th percetiles of age,
# which are 33.8, 42, and 51 as the internal knots.
fit.splines2 = lm(wage~ns(age,df=4),data=Wage)
summary(fit.splines2)
# Predict wage based on the age grid
pred.Splines2 = predict(fit.splines2, newdata=list(age=age.grid), se=TRUE)
# Plot the prediction and confidence intervals
lines(age.grid,pred.Splines2$fit, col="red", lwd=2)
se.bands2 = cbind(pred.Splines2$fit+2*pred.Splines2$se.fit, pred.Splines2$fit-2*pred.Splines2$se.fit)
matlines(age.grid,se.bands2, col="red", lty=2)
# Fitting the smooth spline by using the smooth.spline() function
# We use 16 degree of freedom to determines the value of lamda
fit.SS = smooth.spline(age, wage, df=16)
# We then select the smoothness level by cross-validation with cv=TRUE
fit.SS2 = smooth.spline(age, wage, cv=TRUE)
names(fit.SS2)
# The CV selected degree of freedom = 6.794596
fit.SS2$df
# Plot the two fitted model for comparison
plot(age,wage,col="gray")
lines(fit.SS, col="red", lwd=2)
lines(fit.SS2, col="blue", lwd=2)
#### Local Regression Lines #### loess()
# span = the neighborhood it should consider for analysis
fit.LRL = loess(wage~age, span=0.2, data=Wage)
Predict.LRL = predict(fit.LRL, newdata=data.frame(age=age.grid)) #we shall use data.frame instead of list
fit.LRL2 = loess(wage~age, span=0.5, data=Wage)
Predict.LRL2 = predict(fit.LRL2, newdata=data.frame(age=age.grid))
plot(age, wage, col="gray")
lines(age.grid,Predict.LRL, col="blue", lwd=2)
lines(age.grid,Predict.LRL2, col="red", lwd=2)
library(gam)
# The convenience for fitting a GAM model is that we can mix with different non-linear models
# s() is smoothing spline in GAMs library
# ns() is natural spline in GAMs library
# lo() is local regression in GAMs libarary
gam1 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
summary(gam1)
# Plot the prediction
plot(gam1, se=TRUE, col="blue")  # not an impressive representation"
par(mfrow=c(1,3)) # Show 3 outcomes 1 for each variable  year, age, education
plot.gam(gam1,se=TRUE, col="red")
plot(gam1,se=TRUE, col="red")
### ANOVA IN GAM ###
gam.V1 = gam(wage~s(age,5)+education, data=Wage)
gam.V2 = gam(wage~year+s(age,5)+education, data=Wage)
gam.V3 = gam(wage~ns(year,4)+s(age,5)+education, data=Wage)
anova(gam.V1, gam.V2, gam.V3, test="F")
# Make predictions with the best model
PredictGam = predict(gam.V2, newdata=Wage)
PredictGam
##### Adding Local regression fits by lo() models
gam.lo = gam(wage~s(year,df=4)+lo(age,span=0.5)+education, data=Wage)
summary(gam.lo)
##### Finally that is the time for logistic regression in GAM #####
## You can use I() and don't forget to set family to binomial
gam.lr = gam(I(wage>250)~year+s(age,df=5)+education, family=binomial)
summary(gam.lr)
plot(gam.lr, se=TRUE, col="green")
