attach(Carseats)
library(randomForest)
set.seed(1)
NumberofObservations = nrow(Carseats)
NumberofObservations # that is 400
SplitofTrainTest = 0.5 # let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
# train = sample(1:400, 200)
test = -train
trainingData = Carseats[train,]
testingData  = Carseats[test,]
Testing_outcome = Sales[test]
# Check the dimension of the data set
dim(Carseats)
# Use randomForest() function to build a bagging model,
# Remember the bagging model is a special case of Random Forest with m = P
# where m is the max number of predictors use for tree spliting
# and P is the total number of predictors in the data set.
# Use the arguments mtry and importance in the randomForest() function
bag.car = randomForest(Sales~., data=trainingData, mtry=10, importance=TRUE)
#After training the model, make prediction of the response variable with the testing data set.
bag.car.pred = predict(bag.car, newdata=testingData)
# Calcuate the testing mean squared error (MSE)
mean((bag.car.pred-Testing_outcome)^2)
plot(bag.car.pred, Testing_outcome)
abline(0,1)
abline(0,1, color="red")
abline(0,1, color=red)
for n in c(1:11):
print n
for (n in c(1:11)){
print(n)
}
# book's Libraray
library(ISLR)
# The dataset is from ISLR Library
attach(Carseats)
# Import the libraries for analysis
library(randomForest)
# you can set.seed to any number
set.seed(1)
NumberofObservations = nrow(Carseats)
NumberofObservations # that is 400
SplitofTrainTest = 0.5 # let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
# train = sample(1:400, 200)
test = -train
trainingData = Carseats[train,]
testingData  = Carseats[test,]
Testing_outcome = Sales[test] # The is the our test outcomes
for (n in(1:11)){
rf.carseats[n] = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats[n], newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - test)^2)
}
rf.carseats <- c()
for (n in(1:11)){
rf.carseats[n] = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats[n], newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - test)^2)
}
for (n in(1:11)){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats, newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - test)^2)
}
MSE.rf.carseats <- c()
for (n in(1:11)){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats, newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - test)^2)
}
mtry <- c(1:11)
plot(mtry, MSE.rf.carseats)
for (n in(1:11)){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
rf.carseats
yhat = predict(rf.carseats, newdata=testingData)
yhat
MSE.rf.carseats[n] = mean((yhat - test)^2)
}
randomForest(Sales~., data=trainingData, mtry=1)
randomForest(Sales~., data=trainingData, mtry=2)
for (n in(1:11)){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats, newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - Testing_outcome)^2)
}
for (n in c(1:11)){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats, newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - Testing_outcome)^2)
}
mtry <- c(1:11)
remove(n)
for (n in mtry){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats, newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - Testing_outcome)^2)
}
?Carseats
dim(Carseats)
mtry <- c(1:10)
for (n in mtry){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats, newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - Testing_outcome)^2)
}
plot(mtry, MSE.rf.carseats)
mtry <- c(1:10)
plot(mtry, MSE.rf.carseats)
MSE.rf.carseats <- c()
mtry <- c(1:10)
for (n in mtry){
rf.carseats = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats, newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - Testing_outcome)^2)
}
plot(mtry, MSE.rf.carseats)
for (n in mtry){
rf.carseats[n] = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats[n], newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - Testing_outcome)^2)
}
rf.carseats <- c()
MSE.rf.carseats <- c()
mtry <- c(1:10)
for (n in mtry){
rf.carseats[n] = randomForest(Sales~., data=trainingData, mtry=n)
yhat = predict(rf.carseats[n], newdata=testingData)
MSE.rf.carseats[n] = mean((yhat - Testing_outcome)^2)
}
rf.carseats <- c()
for (n in mtry){
rf.carseats[n] = randomForest(Sales~., data=trainingData, mtry=n)
}
rf.carseats[1]
rf.carseats <- list()
for (n in mtry){
rf.carseats[n] = randomForest(Sales~., data=trainingData, mtry=n)
}
rf.carseats[1]
# Loading the required libraries for the lab
require(ISLR)
attach(Wage)
# In this example, we are tyring to predict wages using age from the dataset
# First fit the model using the basic command for polynomial regression
fit.Poly.Basic = lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
summary(fit.Poly.Basic)
# We can also use the poly() function to fit the same model
fit.Poly =lm(wage~poly(age,4,raw=TRUE),data=Wage)
summary(fit.Poly)
# Note that if we do not define raw=TRUE, the model will still have the same fitted values
# However, the coefficients for each term cannot be interpreted as usual
# In the orthogonal polynomial form, only the p-value can be interpreted
fit.Poly.Orth =lm(wage~poly(age,4),data=Wage)
summary(fit.Poly.Orth)
# Let's create a grid of valuesfor age for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
preds = predict(fit.Poly,newdata = list(age=age.grid),se = TRUE)
names(preds)
se.bands = cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
# Plot the data dn add the fit from the degree-4 polynomial
plot(age, wage, col="darkgray")
lines(age.grid, preds$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid, se.bands, col="blue", lty=2)  #lty shows type of line lty = 2 is used for broken lines
#### Using Anova in Nested sequence of Models ####
fita <- lm(wage~education, data=Wage)
fitb <- lm(wage~education + age, data=Wage)
fitc <- lm(wage~education + poly(age,2), data=Wage)
fitd <- lm(wage~education + poly(age,3), data=Wage)
fite <- lm(wage~education + poly(age,4), data=Wage)
# The p-value indicates the model with the prior model
# The p-vlaue is significant means the prior model is not sufficient
# or the current model is better then the prior model
anova(fita,fitb,fitc,fitd,fite)
#### Logistic Regression ####
# Fitting the logistic regression model on binary response, wage,
# using wrapper I() to create binary response, over 250 (True) and below or equal 250 (False)
# on age with polynomial degree 3
fit.Logistics <- glm(I(wage>250)~poly(age,3), data=Wage, family=binomial)
summary(fit.Logistics)
# Predict the response by the age grid created earlier
preds.Logistics <- predict(fit.Logistics, newdata=list(age=age.grid), se=TRUE)
names(preds.Logistics)
# Create the confidence intervals for the prediction
se.bands <- preds.Logistics$fit+cbind(fit=0, lower=-2*preds.Logistics$se.fit, upper=2*preds.Logistics$se.fit)
se.bands[1:5,]
# Convert logistics to probabilities
prob.bands <- exp(se.bands)/(1+exp(se.bands))
prob.bands[1:5,]
# Plot the predictions with confidence intervals
matplot(age.grid, prob.bands, col="blue", lwd=c(2,1,1), lty=c(1,2,2), type="l")
#### Fitting step functions ####
# In order to fit a step function, we need to use the cut() function
?cut
table(cut(age,4))
fit.Step <- lm(age~cut(age,4), data=Wage)
summary(fit.Step)
table(cut(age, c(30, 50, 60)))
table(cut(age, c(30, 50, 60, 65)))
table(cut(age, c(30, 50, 60, 65, 80)))
tabel(cut(age, 4))
table(cut(age, 4))
# Loading the required libraries for the lab
require(ISLR)
attach(Wage)
?Wage
# In this example, we are tyring to predict wages using age from the dataset
# First fit the model using the basic command for polynomial regression
fit.Poly.Basic <- lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
summary(fit.Poly.Basic)
# We can also use the poly() function to fit the same model
fit.Poly <- lm(wage~poly(age,4, raw=TRUE),data=Wage)
summary(fit.Poly)
# Note that if we do not define raw=TRUE, the model will still have the same fitted values
# However, the coefficients for each term cannot be interpreted as usual
# In the orthogonal polynomial form, only the p-value can be interpreted
fit.Poly.Orth <- lm(wage~poly(age,4), data=Wage)
summary(fit.Poly.Orth)
# Let's create a grid of values for age for prediction
agelims <- range(age)
agelims
agelims
age.grid <- seq(agelims[1], to=agelims[2])
age.grid
preds <- predict(fit.Poly,newdata=list(age=age.grid), se=TRUE)
names(preds)
preds
se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
# Plot the data dn add the fit from the degree-4 polynomial
plot(age, wage, col="darkgray")
lines(age.grid, preds$fit, lwd=2, col="blue") #lwd is linewidth
matlines(age.grid, se.bands, col="red", lty=2)  #lty shows type of line lty = 2 is used for broken lines
#### Using Anova in Nested sequence of Models ####
fita <- lm(wage~education, data=Wage)
fitb <- lm(wage~education + age, data=Wage)
fitc <- lm(wage~education + poly(age,2), data=Wage)
fitd <- lm(wage~education + poly(age,3), data=Wage)
fite <- lm(wage~education + poly(age,4), data=Wage)
# The p-value indicates the model with the prior model
# The p-vlaue is significant means the prior model is not sufficient
# or the current model is better then the prior model
anova(fita,fitb,fitc,fitd,fite)
#### Logistic Regression ####
# Fitting the logistic regression model on binary response, wage,
# using wrapper I() to create binary response, over 250 (True) and below or equal 250 (False)
# on age with polynomial degree 3
fit.Logistics <- glm(I(wage>250)~poly(age,3), data=Wage, family=binomial)
summary(fit.Logistics)
# Predict the response by the age grid created earlier
preds.Logistics <- predict(fit.Logistics, newdata=list(age=age.grid), se=TRUE)
names(preds.Logistics)
# Create the confidence intervals for the prediction
se.bands <- preds.Logistics$fit+cbind(fit=0, lower=-2*preds.Logistics$se.fit, upper=2*preds.Logistics$se.fit)
se.bands[1:5,]
# Convert logistics to probabilities
prob.bands <- exp(se.bands)/(1+exp(se.bands))
prob.bands[1:5,]
# Plot the predictions with confidence intervals
matplot(age.grid, prob.bands, col="Green", lwd=c(2,1,1), lty=c(1,2,2), type="l")
range(wage)
plot(wage)
#### Fitting step functions ####
# In order to fit a step function, we need to use the cut() function
?cut
table(cut(age,4))
fit.Step <- lm(wage~cut(age,4), data=Wage)
summary(fit.Step)
table(cut(age, c(17.9, 35, 50, 65, 80.1)))
pairplot(Auto)
PairPlot(Auto)
pairs(Auto)
Auto
# Validation Set Approach
require(ISLR)
require(boot)
pairs(Auto)
?Auto
attach(Auto)
set.seed(1)
train <- sample(dim(Auto)[1], 300)
train
fit1 <- lm(mpg~horsepower, data=Auto, subset=train)
summary(fit1)
fit2 <- lm(mpg~horsepower+weight, data=Auto, subset=train)
summary(fit2)
fit3 <- lm(mpg~horsepower+weight+displacement, data=Auto, subset=train)
summary(fit3)
test <- -train
trainData = Auto[train,]
testData  = Auto[test,]
testOutcome = Auto$mpg[test]
fit1 <- lm(mpg~horsepower, data=Auto, subset=train)
summary(fit1)
fit2 <- lm(mpg~horsepower+weight, data=trainData)
summary(fit2)
fit3 <- lm(mpg~horsepower+weight+displacement, data=Auto[train,])
summary(fit3)
# Model 1 MSEs
# Training MSE
mean((Auto$mpg-predict(fit1, Auto))[train]^2)
# Testing MSE
mean((Auto$mpg-predict(fit1, Auto))[-train]^2)
?predict
trainOutcome <- Auto$mpg[Train]
trainOutcome <- Auto$mpg[Train]
trainOutcome <- Auto$mpg[train]
# Model 1 MSEs
# Training MSE
fit1_trainMSE <- mean((Auto$mpg-predict(fit1, Auto))[train]^2)
fit1_trainMSE
# Testing MSE
fit1_testMSE <- mean((Auto$mpg-predict(fit1, Auto))[-train]^2)
fit1_testMSE
# Model 2 MSEs
# Training MSE
fit2_trainPred = predict(fit2, newdata=trainData)
fit2_trainMSE = mean((fit2_trainPred - testOutcome )^2)
fit2_trainMSE = mean((fit2_trainPred - trainOutcome )^2)
fit2_trainMSE
# Testing MSE
fit2_testPred = predict(fit2, newdata=testData)
fit2_testMSE = mean((fit2_testPred - testOutcome )^2)
fit2_testMSE
# Model 3 MSEs
# Training MSE
fit3_trainPred <- predict(fit3, Auto)[train]
fit3_trainMSE <- mean((fit3_trainPred - trainOutcome)^2)
fit3_trainMSE
# Test MSE
fit3_testPred <- predict(fit3, Auto)[-train]
fit3_testMSE <- mean((fit3_testPred - testOutcome)^2)
fit3_testMSE
# Create data frame to show result
models <- c("fit1", "fit2", "fit3")
trainMSEs <- c(fit1_trainMSE, fit2_trainMSE, fit3_trainMSE)
testMSEs <- c(fit1_testMSE, fit2_testMSE, fit3_testMSE)
results <- data.frame(models, trainMSEs, testMSEs)
results
barplot(results)
plot(results)
barplot(tabel(results$trainMSEs, results$testMSEs))
barplot(table(results$trainMSEs, results$testMSEs))
barplot(results$models, table(results$trainMSEs, results$testMSEs))
barplot(results, beside=TRUE)
barplot(results$models, beside=TRUE)
barplot(results$models, results$trainMSEs, beside=TRUE)
barplot(trainMSEs, beside=TRUE)
plot(models, trainMSEs)
plot(trainMSEs, models)
barplot(table(trainMSEs))
barplot(table(model,trainMSEs))
barplot(table(models,trainMSEs))
barplot(table(trainMSEs, models))
table(trainMSEs, testMSEs)
list(trainMSEs, testMSEs)
barplot(list(trainMSEs, testMSEs))
c(trainMSEs, testMSEs)
barplot(c(trainMSEs, testMSEs))
barplot(c(trainMSEs, testMSEs), beside=TRUE)
barplot(c(trainMSEs, testMSEs),
legend=results, beside=TRUE)
barplot(c(trainMSEs, testMSEs),
legend=models, beside=TRUE)
barplot(c(trainMSEs, testMSEs),
legend=models, col=c("green", "blue", "red") beside=TRUE)
barplot(c(trainMSEs, testMSEs),
legend=models, col=c("green", "blue", "red"),
beside=TRUE)
barplot(c(trainMSEs, testMSEs),
legend=models, col=c("green", "blue", "red"))
axis(c("Train MSE", "Test MSE"))
plot(trainMSEs, models)
x <- c(2.3, 3.3. 4.5)
plot(x, models)
x <- c(2.3, 3.3. 4.5)
x <- c(2.3, 3.3, 4.5)
plot(x, models)
x <- c(1, 2, 3)
plot(x, trainMSEs)
plot(x, trainMSEs, type=b, col="blue")
plot(x, trainMSEs, type=b, col="blue")
plot(x, trainMSEs, type="b", col="blue")
line(x, testMSEs, col="red")
line(x, testMSEs, type="b", col="red")
lines(x, testMSEs, type="b", col="red")
lines(x, trainMSEs, type="b", col="red")
plot(x, testMSEs, type="b", col="blue")
lines(x, trainMSEs, type="b", col="red")
# LOOCV Approach
fit1 <- glm(mpg~horsepower, data=Auto)
summary(fit1)
fit2 <- glm(mpg~horsepower+weight, data=Auto)
summary(fit2)
fit3 <- glm(mpg~horsepower+weight+displacement, data=Auto)
summary(fit3)
# Calculate the LOOCV MSE for each model
# Model 1 LOOCV MSE
fit1_loocv <- cv.glm(Auto, fit1)$delta
fit1_loocv
fit1_loocv$delta
cv.glm(Auto, fit1)$delta
cv.glm(Auto, fit1)$delta[1]
# Calculate the LOOCV MSE for each model
# Model 1 LOOCV MSE
fit1_loocv <- cv.glm(Auto, fit1)$delta[1]
fit1_loocv
# Model 2 LOOCV MSE
loocv = function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
fit2_loocv <- loocv(fit2)
fit2_loocv
col=c("blue", "red")
legend(1, 95, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"))
legend(legend=c("testMSE", "trainMSE"),
col=c("blue", "red"))
legend(2.5, 22, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"))
legend(3, 24, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"))
legend(2.7, 24, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"))
# Using a bar chart to demonstrate the training and test MSEs
barplot(c(trainMSEs, testMSEs),
legend=models, col=c("green", "blue", "red"),
lty=1)
# Using a bar chart to demonstrate the training and test MSEs
barplot(c(trainMSEs, testMSEs),
legend=models, col=c("green", "blue", "red"))
# Plot the training and testing MSEs
plot(x, testMSEs, type="b", col="blue")
lines(x, trainMSEs, type="b", col="red")
legend(2.7, 24, legend=c("testMSE", "trainMSE"),
col=c("blue", "red")
lty=1)
legend(2.7, 24, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"),
lty=1)
legend(2.6, 24, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"),
lty=1)
# Plot the training and testing MSEs
plot(x, testMSEs, type="b", col="blue")
lines(x, trainMSEs, type="b", col="red")
legend(2.6, 24, legend=c("testMSE", "trainMSE"),
col=c("blue", "red"),
lty=1)
# Model 3 LOOCV MSE
fit3_loocv <- loocv(fit3)
fit3_loocv
fit2_loocv
loocv_MSEs <- c(fit1_loocv, fit2_loocv, fit3_loocv)
# Adding the LOOCV MSEs to the previous graph
lines(x, loocv_MSEs, type="b", col="green")
legend(2.6, 24, legend=c("testMSE", "trainMSE", "LOOCVMSE"),
col=c("blue", "red", "green"),
lty=1)
legend(2.5, 24, legend=c("testMSE", "trainMSE", "LOOCVMSE"),
col=c("blue", "red", "green"),
lty=1)
loocv_results <- data.frame(models, loocv_MSEs)
loocv_results
# 5-Folds & 10-Folds Cross-Validation
# 5-Fold Cross-Validation
# Model 1 MSE
fit1_5fold <- cv.glm(Auto, fit1, K=5)$delta[1]
fit1_5fold
# Model 1 MSE
fit2_5fold <- cv.glm(Auto, fit2, K=5)$delta[1]
fit2_5fold
# Model 1 MSE
fit3_5fold <- cv.glm(Auto, fit3, K=5)$delta[1]
fit3_5fold
# 10-Fold Cross-Validation
# Model 1 MSE
fit1_10fold <- cv.glm(Auto, fit1, K=5)$delta[1]
fit1_10fold
# Model 1 MSE
fit2_10fold <- cv.glm(Auto, fit2, K=5)$delta[1]
fit2_10fold
# Model 1 MSE
fit3_10fold <- cv.glm(Auto, fit3, K=5)$delta[1]
fit3_10fold
5fold_MSEs <- c(fit1_5fold, fit2_5fold, fit3_5fold)
10fold_MSEs <- c(fit1_10fold, fit2_10fold, fit3_10fold)
fold5_MSEs <- c(fit1_5fold, fit2_5fold, fit3_5fold)
fold10_MSEs <- c(fit1_10fold, fit2_10fold, fit3_10fold)
fold_results <- data.frame(models, fold5_MSEs, fold10_MSEs)
fold_results
# Plot the results to the same graph
lines(x, fold5_MSEs, type="b", col="purple")
lines(x, fold10_MSEs, type="b", col="brown")
legend(2.5, 24, legend=c("testMSE", "trainMSE", "LOOCVMSE", "5-Fold MSE", "10-Fold MSE"),
col=c("blue", "red", "green", "purple", 'brown'),
lty=1)
# Plot the training and testing MSEs
plot(x, testMSEs, type="b", col="blue")
lines(x, trainMSEs, type="b", col="red")
results
# Adding the LOOCV MSEs to the previous graph
lines(x, loocv_MSEs, type="b", col="green")
# Plot the results to the same graph
lines(x, fold5_MSEs, type="b", col="purple")
lines(x, fold10_MSEs, type="b", col="brown")
legend(2.5, 24, legend=c("testMSE", "trainMSE", "LOOCVMSE", "5-Fold MSE", "10-Fold MSE"),
col=c("red", "blue", "green", "purple", 'brown'),
lty=1)
