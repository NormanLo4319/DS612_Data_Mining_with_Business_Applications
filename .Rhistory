diaDBSource <- src_sqlite("C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
# Importing the magritter package for pipe operation
library(magrittr)
# After loading the dplyr package, the data set will be displayed as tbl object
library(dplyr)
# A preferred method is using the interp() function in the lazyeval pacakge
library(lazyeval)
# If using dplyr 0.6.0 version, we can combine the use of filter() function and UQ() function from rlang pacakge
library(rlang)
# First, we need to build the connection with the database
# install.packages("RSQLite")
diaDBSource <- src_sqlite("C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
# First, we need to build the connection with the database
# install.packages("RSQLite")
diaDBSource <- src_sqlite("C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
# First, we need to build the connection with the database
# install.packages("RSQLite")
library(RSQLite)
diaDBSource <- src_sqlite("C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
diaDBSource <- src_sqlite("C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db", create=TRUE)
diaDBSource <- src_sqlite("C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
environment(src_sqlite)
environment(src_sql)
# First, we need to build the connection with the database
# install.packages("RSQLite")
library(RSQLite)
# First, we need to build the connection with the database
# install.packages("RSQLite")
library(dplyr)
# If using dplyr 0.6.0 version, we can combine the use of filter() function and UQ() function from rlang pacakge
library(rlang)
# If using dplyr 0.6.0 version, we can combine the use of filter() function and UQ() function from rlang pacakge
install.packages(rlang)
# If using dplyr 0.6.0 version, we can combine the use of filter() function and UQ() function from rlang pacakge
install.packages("rlang")
library(rlang)
dplyr
# First, we need to build the connection with the database
# install.packages("RSQLite")
library("dplyr")
# First, we need to build the connection with the database
# install.packages("RSQLite")
library("RSQLite")
diaDBSource <- src_sqlite("C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
diaDBSource
diaDBSource2 <- DBI::dbConnet(RSQLite::SQLite(),
"C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
diaDBSource2 <- DBI::dbConnet(RSQLite::SQLite(), "C:/Users/lokma/Desktop/R_Applications/SQLite/diamonds.db")
# Inside the database, there are 3 data frames, 'diamonds', 'DiamondColors', and 'sqlite_stat1'
# For each data frame, we need to appoint to that specific data frame at a time
diaTab <- tbl(diaDBSource, 'diamonds')
diaTab
# Note: It looks like a data.frame, but it is actually the database table
# Majority of the calcuation of this tbl is done within the database
diaTab %>% group_by(cut) %>% dplyr::summarize(Price=mean(price))
diaTab %>%
group_by(cut) %>%
dplyr::summarize(Price=mean(price), Carat=mean(Carat))
install.packages("DBI")
install.packages("DBI")
install.packages("DBI")
install.packages("DBI")
install.packages("DBI")
install.packages("DBI")
install.packages("DBI")
install.packages("odbc")
library(odbc)
sort(unique(odbcListDrivers()[[1]]))
con <- dbConnect(odbc(), "DSN name")
con <- dbConnect(odbc(),
Driver = "SQL Server",
Server = "localhost\\SQLEXPRESS",
Database = "datawarehouse",
Trusted_Connection = "True")
# Similar to base R rbind() and cbind(), dplyr has the similar functions bind_rows() and bind_cols().
# The two are not exactly the same, which dplyr functions only apply to data.frame or tibble.
# Base R functions can be more genearlly applied to combine vectors into matrices or data.frame.
# Import dplyr package
library(dplyr)
library(tibble)
# Create a tibble with two columns
sportLeague <- tibble(sport=c("Hockey", "Baseball", "Foodball", "Basketball"),
league=c("NHL", "MLB", "NFL", "NBA"))
# Create a tibble with one column
trophy <- tibble(trophy=c("Stanley Cup", "Commissioner's Trophy",
"Vince Lombardi Trophy", "Larry O'Brien Trophy"))
# Combine the two tibbles into one
trophiesl <- bind_cols(sportLeague, trophy)
# Use tribble create anotehr tibble
trophies2 <- tribble(
~sport, ~league, ~trophy,
"Golf", "PGA", "Wanamaker Trophy",
"Tennis", "Wimbledon", "Wimbledon Trophy"
)
# Combine the two tibbles into one
trophies1 <- bind_cols(sportLeague, trophy)
# Combine the trophies1 with trophies2 (adding new row)
trophies <- bind_rows(trophies1, trophies2)
trophies
library(readr)
colorsURL <- 'http://www.jaredlander.com/data/DiamondColors.csv'
diamondColors <- read_csv(colorsURL)
diamondColors
data(diamonds, package='ggplot2')
unique(diamonds$color)
library(dplyr)
# Using left_join() with column 'color' in diamonds and 'Color' in diamondColors.
left_join(diamonds, diamondColors, by=c('color'='Color'))
?left_join
diamonds
dtype(diamonds)
type(diamonds)
class(diamonds)
# If we only want to extract some specific columns, we can also use the pipe operator
# e.g. only select carat, color, price, description, and details columns after join
left_join(diamonds, diamondColors, by=c('color'='Color')) %>%
select(carat, color, price, Description, Details)
# After Join:
left_join(diamonds, diamondColors, by=c('color'='Color')) %>%
distinct(color, Details)  # only 7 colors were matched to the left tbl
# Before Join:
diamondColors %>% distinct(Color, Description)  # total 10 colors in the original tbl
# Using a right_join() function, we are keeping all of the existing rows from the right tbl
# and match with the rows in the left tbl.
# In right join case, the joined tbl contains more rows than the left tbl (diamonds).
# Before Join:
diamonds %>% nrow
# After Join:
right_join(diamonds, diamondColors, by=c('color'='Color')) %>%
nrow
inner_join(diamonds, diamondColors, by=c('color'='Color'))
# In this example, the inner_join() should return the same rows as the left_join() because
# the right tbl has some rows that cannot be match with the left tbl.
all.equal(left_join(diamonds, diamondColors, by=c('color'='Color')),
inner_join(diamonds, diamondColors, by=c('color'='Color')))
# full_join() (usually called "Outter Join") will joined tbl with with all the rows from the two tbls,
# even without match.
full_join(diamonds, diamondColors, by=c('color'='Color')))
# full_join() (usually called "Outter Join") will joined tbl with with all the rows from the two tbls,
# even without match.
full_join(diamonds, diamondColors, by=c('color'='Color'))
# In this example, the full_join() should return the same rows as the right_join() because
# the right tbl has 7 rows that cannot be match to the left tbl and will be included.
all.equal(right_join(diamonds, diamondColors, by=c('color'='Color')),
full_join(diamonds, diamondColors, by=c('color'='Color')))
# semi_join() returns only the first match from the left tbl to the right tbl, which is more like sorting.
# If we set "diamondColors" as the left tbl, only the matched color in "diamonds" tbl will be returned.
semi_join(diamondColors, diamonds, by=c('Color'='color'))
# anti_join() is the opposite of the semi_join(), which returns the unmatch rows from the left tbl.
anti_join(diamondColors, diamonds, by=c('Color'='color'))
# We can also apply the filter() and unique() to achieve the semi_join() or anti_join(), but the later ones are preferred
# when dealing with data.frame.
diamondColors %>% filter(Color %in% unique(diamonds$color))
diamondColors %>% filter(!Color %in% unique(diamonds$color))
library(readr)
emontion <- read_tsv('http://www.jaredlander.come/data/reaction.txt')
emontion <- read_tsv('http://www.jaredlander.com/data/reaction.txt')
emotion
emotion <- read_tsv('http://www.jaredlander.com/data/reaction.txt')
emotion
# Note that the return tibble is a wide format.  We can tranform the data into a long format tibble by using gather(),
# which is similar to melt() in reshape2 package.
# We will stack the "Age", "React", and "Regulate" into a single column called "Measurement"
# A new column will also be created and called "Type" to identify the column names being stacked.
library(tidyr)
emotion %>%
gather(key=Type, value=Measurement, Age, BMI, React, Regulate)
# The new tbl will be sorted by 'Type', which is the key defined in gather().
# It would be difficult to identify the changes of the data, so we can arrange it by ID.
emotionLong <- emotion %>%
gather(key=Type, value=Measurement, Age, BMI, React, Regulate) %>%
arrange(ID)
emotionLong
# We can also appoint the columns to be included in the return tbl, or using "-" to excluded in the return tbl.
# e.g.
emotion %>%
gather(key=Type, value=Measurement, -ID, -Test, -Gender) %>%
arrange(ID)
identical(
emotion %>%
gather(key=Type, value=Measurement, Age, BMI, React, Regulate) %>%
arrange(ID),
emotion %>%
gather(key=Type, value=Measurement, -ID, -Test, -Gender) %>%
arrange(ID)
)
# Opposite to gather() is spread(), which is similar to dcast() in reshape2 package.
# spread() can transform the long format data into wide format data.
# In general, it can break the stacked data into columns.
# e.g. Suppose we are interested to break the emotionLong data into it's original form.
emotionLong %>%
spread(key=Type, value=Measurement)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
require(ISLR)
set.seed(1)
y=rnorm(100)
x=rnorm(100)
y=x-2*x^2+rnorm(100)
plot(x,y)
library(boot)
data <- data.frame(x,y)
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
cv.error=rep(0,4)
degree=1:4
for(d in degree){
glm.fit=glm(y~poly(x,d))
cv.error[d]=loocv(glm.fit)
}
cv.error
plot(degree,cv.error,type="b")
glm.fit.1=lm(y~poly(x,1))
glm.fit.2=lm(y~poly(x,2))
glm.fit.3=lm(y~poly(x,3))
glm.fit.4=lm(y~poly(x,4))
summary(glm.fit.1)
summary(glm.fit.2)
summary(glm.fit.3)
summary(glm.fit.4)
library(MASS)
attach(Boston)
mu.hat <- mean(medv)
mu.hat
NumberOfObservations = dim(Boston)[1]
NumberOfObservations
S.d.Hat.Mu.Hat = sd(medv)/(NumberOfObservations ^ .5)
S.d.Hat.Mu.Hat
require(boot)
muHat.fn=function(data, index){
X = data$medv[index]
return(mean(X))
}
boot.out=boot(Boston,muHat.fn,R=1000)
boot.out
MuHat.Med = median(medv)
MuHat.Med
muHat.Med.fn=function(data, index){
X = data$medv[index]
return(median(X))
}
boot.out=boot(Boston,muHat.Med.fn,R=1000)
boot.out
?quantile
quantile(medv,0.1)
install.packages("tinytex")
install.packages("tinytex")
tinytex::install_tinytex()
# In this section, we are going to use the tree-based methods for regresssion and classification.
# We first need to install the 'randomForest' and 'ISLR' packages
install.packages("randomForest")
install.packages("ISLR")
install.packages("tree")
# Import the libraries for analysis
library(randomForest)
library(ISLR)
library(tree)
library(MASS)
# We are going to use the Boston data set from MASS library
?Boston
# Splitting data into training and testing sets (50/50)
set.seed(1)
NumberofObservations = dim(Boston)[1]
SplitofTrainTest = 0.5 #let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Boston[train,]
testingData  = Boston[test,]
Testing_outcome = Boston$medv[test]
?randomForest
# Bagging is simply a special case of random forest with m = p (All predictors are used to grow trees)
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE)
# mtry is the nymber of variables randomly sampled as candidates at each slpit.
# The defult is sqrt(p) in classification and p/3 in regression
# The defult number of trees is 500
bag.boston
names(bag.boston)
summary(bag.boston)
# Using the importance() function to check the importance of each variable
importance(bag.boston)
# We can plot these importance measures with the varImpPlot() function
varImpPlot(bag.boston)
# Make prediction with the trained model by passing in the testing dataset
predict.bag = predict(bag.boston, newdata=testingData)
# Plot the prediction and testing outcome
plot(predict.bag,Testing_outcome)
abline(0,1)
# Calculate the MSE for the bagging model
MSE.bag = mean((predict.bag - Testing_outcome )^2)
MSE.bag
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=25)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.25Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.25Trees #Our error increased
# What if we choose less than 500 trees?
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=50)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.25Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.25Trees #Our error increased
# What if we choose less than 500 trees?
# Let's build a forest with 50 trees
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=50)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.50Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.50Trees #Our error increased
# Using a for loop to run different size of mtry (number of predictors to be consider for each split of the tree)
MSE.Rf=rep(0,13)
for(d in 1:13){
rf.boston = randomForest(medv~., data=trainingData, mtry=d, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[d] = mean((predict.rf- Testing_outcome )^2)
}
MTRY = c(1:13)
# Plot the MSE for each size of ntry
plot(MTRY,MSE.Rf,type="b",col="red")
min(MSE.Rf)
# mtry=3 created the minimum error - if you repeat this over and over you may get another miminizers such as 4 or 5
rf.boston = randomForest(medv~., data=trainingData, mtry=3, importance=TRUE)
# Getting the prediction from the best model
predict.rf = predict(rf.boston, newdata=testingData)
# Calculate the MSE from the best model
MSE.Rf = mean((predict.rf- Testing_outcome )^2)
MSE.Rf
# Check the importance measures
importance(rf.boston)
# Plot the importance measures
varImpPlot(rf.boston)
library(gbm)
set.seed(1)
# If you are running regression problems then use distribution = "gaussian". If you are working on
# binary classfiication problems, then use distribution = "bernoulli"
# The default value of Lambda is 0.001
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4)
boost.boston
summary(boost.boston)
summary(Boston$medv)
# Parial dependence plots for rm and lstat
# These plots illustrate the marginal effect of the selected variables on the response (medv) after
# integerating out the other variables. as we expect medv is increasing with rm and decreasing with lstat
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Get the prediction from the boosting model
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
# Calculate the MSE of the boosting model
mean((predict.boost-Testing_outcome )^2)
# Let's Change Lambda
Lambda = c(.00001,0.0001,0.001,.01,0.1,.15,.2)
Counter = 1
MSE.Boost = rep(0,7)
# Using a for loop to check different values for Lamda
for(d in Lambda){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4, shrinkage=d)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
MSE.Boost[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# The miminum happened at Lambda = 0.01
plot(Lambda,MSE.Boost,type="b",col="red")
# 17.79736 less than random forest
min(MSE.Boost)
# Now let's fix Lambda and change size of the tree
TreeSize = c(50,100,200,400,500,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000)
Counter = 1
MSE.Boost.Tree = rep(0,15)
# Create a for loop to check different tree size with the best Lamda selected (shrinkage=0.01)
for(d in TreeSize){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=d, interaction.depth=4, shrinkage=0.01)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=d)
MSE.Boost.Tree[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# The tree size reaches miminum at 4000
plot(TreeSize,MSE.Boost.Tree,type="b",col="red")
# it seems like 4000 was a very good choice
min(MSE.Boost.Tree)
MSE.Boost.Tree
data.frame(TreeSize,MSE.Boost.Tree)
# Import the libraries for analysis
library(randomForest)
library(ISLR)
library(tree)
library(MASS)
# Splitting data into training and testing sets (50/50)
set.seed(1)
NumberofObservations = dim(Boston)[1]
SplitofTrainTest = 0.5 #let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Boston[train,]
testingData  = Boston[test,]
Testing_outcome = Boston$medv[test]
?randomForest
# Bagging is simply a special case of random forest with m = p (All predictors are used to grow trees)
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE)
# mtry is the number of variables randomly sampled as candidates at each slpit.
# The default is sqrt(p) in classification and p/3 in regression
# The default number of trees is 500
bag.boston
names(bag.boston)
summary(bag.boston)
# Using the importance() function to check the importance of each variable
importance(bag.boston)
# We can plot these importance measures with the varImpPlot() function
varImpPlot(bag.boston)
# Make prediction with the trained model by passing in the testing dataset
predict.bag = predict(bag.boston, newdata=testingData)
# Plot the prediction and testing outcome
plot(predict.bag,Testing_outcome)
abline(0,1)
# Calculate the MSE for the bagging model
MSE.bag = mean((predict.bag - Testing_outcome )^2)
MSE.bag
# What if we choose less than 500 trees?
# Let's build a forest with 50 trees
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=50)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.50Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.50Trees #Our error increased
# What if we choose less than 500 trees?
# Let's build a forest with 50 trees
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=50)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.50Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.50Trees #Our error increased
for(d in 1:13){
rf.boston = randomForest(medv~., data=trainingData, mtry=d, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[d] = mean((predict.rf- Testing_outcome )^2)
}
# Using a for loop to run different size of mtry (number of predictors to be consider for each split of the tree)
MSE.Rf=rep(0,13)
for(d in 1:13){
rf.boston = randomForest(medv~., data=trainingData, mtry=d, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[d] = mean((predict.rf- Testing_outcome )^2)
}
MTRY = c(1:13)
# Plot the MSE for each size of ntry
plot(MTRY,MSE.Rf,type="b",col="red")
min(MSE.Rf)
data.frame(MTRY, MSE.RF)
data.frame(MTRY, MSE.Rf)
# mtry=3 created the minimum error - if you repeat this over and over you may get another miminizers such as 4 or 5
rf.boston = randomForest(medv~., data=trainingData, mtry=3, importance=TRUE)
# Getting the prediction from the best model
predict.rf = predict(rf.boston, newdata=testingData)
# Calculate the MSE from the best model
MSE.Rf = mean((predict.rf- Testing_outcome )^2)
MSE.Rf
# Check the importance measures
importance(rf.boston)
# Plot the importance measures
varImpPlot(rf.boston)
library(gbm)
set.seed(1)
?gbm
# If you are running regression problems then use distribution = "gaussian". If you are working on
# binary classfiication problems, then use distribution = "bernoulli"
# The default value of Lambda is 0.001 (learning rate of the tree model)
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4)
boost.boston
summary(boost.boston)
summary(Boston$medv)
# Parial dependence plots for rm and lstat
# These plots illustrate the marginal effect of the selected variables on the response (medv) after
# integerating out the other variables. as we expect medv is increasing with rm and decreasing with lstat
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
?Boston
# Get the prediction from the boosting model
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
# Calculate the MSE of the boosting model
mean((predict.boost-Testing_outcome )^2)
# Let's Change Lambda
Lambda = c(.00001,0.0001,0.001,.01,0.1,.15,.2)
Counter = 1
MSE.Boost = rep(0,7)
# Using a for loop to check different values for Lamda
for(d in Lambda){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4, shrinkage=d)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
MSE.Boost[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# The miminum happened at Lambda = 0.01
plot(Lambda,MSE.Boost,type="b",col="red")
# 17.79736 less than random forest
min(MSE.Boost)
data.frame(Lambda, MSE.Boost)
# Now let's fix Lambda and change size of the tree
TreeSize = c(50,100,200,400,500,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000)
Counter = 1
MSE.Boost.Tree = rep(0,15)
# Create a for loop to check different tree size with the best Lamda selected (shrinkage=0.01)
for(d in TreeSize){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=d, interaction.depth=4, shrinkage=0.01)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=d)
MSE.Boost.Tree[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# The tree size reaches miminum at 4000
plot(TreeSize,MSE.Boost.Tree,type="b",col="red")
# it seems like 4000 was a very good choice
min(MSE.Boost.Tree)
data.frame(TreeSize,MSE.Boost.Tree)
