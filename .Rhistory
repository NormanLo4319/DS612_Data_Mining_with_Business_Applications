glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
# Compare the prediction with the testing data
Direction.2005 <- Smarket$Direction[!train]
# Present the comparison in a matrix table
table(glm.pred,Direction.2005)
# Calculate the accuracy for the prediction
mean(glm.pred == Direction.2005)
# fit a logistic regression using only the subset of observations of dates before 2005
glm.fits=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data = Smarket, family = binomial, subset = train)
summary(glm.fits)
Smarket.2005
Smarket[!train,]
glm.pred
glm.pred
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005) #!!!!
# fit a logistic regression using only the subset of observations of dates before 2005
glm.fits=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data = Smarket, family = binomial, subset = train)
summary(glm.fits)
glm.probs=predict(glm.fits, Smarket.2005, type = "response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="UP"
glm.pred
mean(glm.pred==Direction.2005)
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
glm.pred
dtype(glm.pred)
type(glm.pred)
str(glm.pred)
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="UP"
str(glm.pred)
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
mean(glm.pred==Direction.2005)
# For Cross Validation, we need to import ISLM and Bootstrap
require(ISLR)
require(boot)
# Plot the variables mpg and horsepower from Auto data set
plot(mpg~horsepower,data=Auto)
# Validation Set Approach:
# First create an index for the subset data from the original dataset.
# For instance, we can create a subset of 300 random sample for the dataset.
set.seed(1)
train <- sample(dim(Auto)[1], 300)
train
# Fit the model with the training set
fit1 <- lm(mpg~horsepower, data=Auto, subset=train)
summary(fit1)
# Calcuate the Mean Square Error for the training set (Train Error)
# MSE = average (yi - yhat)^2
mean((Auto$mpg-predict(fit1, Auto))[train]^2)
# Calculate the Mean Square Error for the testing set (Test Error)
mean((Auto$mpg-predict(fit1, Auto))[-train]^2)
# We can continue to the do the same thing for quadratic and cubic regressions.
fit2 <- lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
summary(fit2)
mean((Auto$mpg-predict(fit2, Auto))[train]^2)
mean((Auto$mpg-predict(fit2, Auto))[-train]^2)
fit3 <- lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
summary(fit3)
mean((Auto$mpg-predict(fit3, Auto))[train]^2)
mean((Auto$mpg-predict(fit3, Auto))[-train]^2)
# We are going to use the cv.glm() function for cross validation
?cv.glm
# We first fit the data into a simple linear model using the glm() function
glm.fit <- glm(mpg~horsepower, data=Auto)
summary(glm.fit)
# We can then use the cross validation function to run glm model for (n - 1) times
# pretty slow (doesnt use formula (5.2) on page 180)
cv.glm(Auto,glm.fit)$delta
##Lets write a simple function to use formula (5.2)
loocv = function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
# Now we use the loocv() function to perform the cross validation
loocv(glm.fit)
# Using a for loop to check the the k-fold cross validation comparison
# e.g.
# n = 400
# k = 10
# in each box, the box will contain a random subset of sample of 40 observations
# Try to use 10 cross validation outcomes
cv.error <- rep(0,10)
# Degree of polynomial from 1 to 10
degree <- 1:10
for(d in degree){
glm.fit <- glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d] <- loocv(glm.fit)
}
cv.error
# Plot the cv error and compare different degrees for cross validation
plot(degree,cv.error,type="b")
# Now, let's try to use the 10-fold cross validation
# Try to use 5 cross validation outcomes with degree of polynomial from 1 to 10
cv.error10 <- rep(0,5)
for(d in degree){
glm.fit <- glm(mpg~poly(horsepower, d), data=Auto)
cv.error10[d] <- cv.glm(Auto,glm.fit, K=10)$delta[1]
}
cv.error10
# Plot the line that represents the degree and cv error
lines(degree, cv.error10, type="b", col="red")
for(d in degree){
glm.fit <- glm(mpg~poly(horsepower, d), data=Auto)
cv.error5[d] <- cv.glm(Auto,glm.fit, K=5)$delta[1]
}
# Try to use 5 cross validation outcomes with degree of polynomial from 1 to 10
cv.error5 <- rep(0,10)
for(d in degree){
glm.fit <- glm(mpg~poly(horsepower, d), data=Auto)
cv.error5[d] <- cv.glm(Auto,glm.fit, K=5)$delta[1]
}
cv.error5
# Plot the line that represents the degree and cv error
lines(degree, cv.error5, type="b", col="blue")
# For Cross Validation, we need to import ISLM and Bootstrap
require(ISLR)
require(boot)
# Plot the variables mpg and horsepower from Auto data set
plot(mpg~horsepower,data=Auto)
# Validation Set Approach:
# First create an index for the subset data from the original dataset.
# For instance, we can create a subset of 300 random sample for the dataset.
set.seed(1)
train <- sample(dim(Auto)[1], 300)
train
# Fit the model with the training set
fit1 <- lm(mpg~horsepower, data=Auto, subset=train)
summary(fit1)
# Calcuate the Mean Square Error for the training set (Train Error)
# MSE = average (yi - yhat)^2
mean((Auto$mpg-predict(fit1, Auto))[train]^2)
# Calculate the Mean Square Error for the testing set (Test Error)
mean((Auto$mpg-predict(fit1, Auto))[-train]^2)
# We can continue to the do the same thing for quadratic and cubic regressions.
fit2 <- lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
summary(fit2)
mean((Auto$mpg-predict(fit2, Auto))[train]^2)
mean((Auto$mpg-predict(fit2, Auto))[-train]^2)
fit3 <- lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
summary(fit3)
mean((Auto$mpg-predict(fit3, Auto))[train]^2)
mean((Auto$mpg-predict(fit3, Auto))[-train]^2)
# We are going to use the cv.glm() function for cross validation
?cv.glm
# We first fit the data into a simple linear model using the glm() function
glm.fit <- glm(mpg~horsepower, data=Auto)
# We can then use the cross validation function to run glm model for (n - 1) times
# pretty slow (doesnt use formula (5.2) on page 180)
cv.glm(Auto,glm.fit)$delta
?lm.influence
##Lets write a simple function to use formula (5.2)
loocv = function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
# Now we use the loocv() function to perform the cross validation
loocv(glm.fit)
# Try to use 10 cross validation outcomes
cv.error <- rep(0,10)
# Degree of polynomial from 1 to 10
degree <- 1:10
for(d in degree){
glm.fit <- glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d] <- loocv(glm.fit)
}
cv.error
# Plot the cv error and compare different degrees for cross validation
plot(degree,cv.error,type="b")
# Now, let's try to use the 10-fold cross validation
# Try to use 5 cross validation outcomes with degree of polynomial from 1 to 10
cv.error10 <- rep(0,10)
for(d in degree){
glm.fit <- glm(mpg~poly(horsepower, d), data=Auto)
cv.error10[d] <- cv.glm(Auto,glm.fit, K=10)$delta[1]
}
cv.error10
# Plot the line that represents the degree and cv error
lines(degree, cv.error10, type="b", col="red")
# Try to use 5-fold cross validation outcomes with degree of polynomial from 1 to 10
cv.error5 <- rep(0,10)
for(d in degree){
glm.fit <- glm(mpg~poly(horsepower, d), data=Auto)
cv.error5[d] <- cv.glm(Auto,glm.fit, K=5)$delta[1]
}
cv.error5
# Plot the line that represents the degree and cv error
lines(degree, cv.error5, type="b", col="blue")
profile
# We are going to use 'Portfolio' data set
?Portfolio
# What is the standard error of alpha?
# We are going to create an alpha function for the analysis
alpha.fn = function(data, index){
X = data$X[index]
Y = data$Y[index]
return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
require(boot)
# Use the alpha.fn() function to calculate the standard error for alpha
alpha.fn(Portfolio,1:100)
# Set a random seed number
set.seed(1)
# Use the alpha.fn() function again on the 100 sample in the data set
alpha.fn(Portfolio,sample(1:100,100,replace=TRUE))
?boot
# Use boot() function for repeated measures by 1000 times
boot.out <- boot(Portfolio,alpha.fn,R=1000)
boot.out
# Plotting the bootstrap output
plot(boot.out)
# Use boot() function for repeated measures by 1000 times
boot.out <- boot(Portfolio,alpha.fn,R=5000)
boot.out
# Plotting the bootstrap output
plot(boot.out)
# Estimateing the Accuracy of a Linear Regression Model
# Create a boostrap function to pass in the data set and index
boot.fn <- function(data, index){
return(coef(lm(mpg~horsepower, data=data, subset=index)))
}
# Using the bootstrap function to estimate the coefficiencts by defining the subset of data.
# For example, we can define to use all 392 observation from the data.
boot.fn(Auto, 1:392)
# Compare it to the coefficienct with the regular lm() function without specifying subset.
coef(lm(mpg~horsepower, data=Auto))
# Now, let's try to set up a random seed and generate a random sample with replacement.
set.seed(1)
boot_s1 <- sample(392, 392, replace=TRUE)
boot_s1
boot.fn(Auto, boot_s1)
boot_s2 <- sample(392, 392, replace=TRUE)
boot_s2
boot.fn(Auto, boot_s2)
boot.lm <- boot(Auto, boot.fn, R=1000)
boot.lm
plot(boot.lm)
r(lm(mpg~hoursepower,data=auto))
?lm
fitted(lm(mpg~hoursepower,data=auto))
summary(lm(mpg~hoursepower,data=auto))
summary(lm(mpg~hoursepower,data=Auto))
summary(lm(mpg~horsepower,data=Auto))
View(fit1)
View(fit1)
names(summary(lm(mpg~horsepower,data=Auto)))
adj.r.square(lm(mpg~horsepower,data=Auto))
summary(lm(mpg~horsepower,data=Auto))$adj.r.square
boot.r <- function(data,index){
return(summary(lm(mpg~horsepower,data=data, subset=index))$adj.r.square)
}
boot.r <- boot(Auto, boot.r, R=5000)
boot.r
plot(boot.r)
plot(boot.lm, index=2)
plot(boot.lm, index=2)
plot(boot.r)
boot.lm <- boot(Auto, boot.fn, R=5000)
boot.lm
plot(boot.lm, index=2)
# Import ISLR library
require(ISLR)
# Export dataset to csv file
write.csv(Smarket, 'Data\\Textbook_Data\\Smarket.csv', row.names=FALSE)
# Plot the pairs correlation of the data
pairs(Smarket,col=Smarket$Direction)
# Logistic Regression
# Use the General Linear Model function to build the logistic regression model.
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
# Print the summary of the fitted model
summary(glm.fit)
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
# In the previous model, we found that the lag1 and lag2 are the best predictors in the model
# Let's build a smaller model with just lag1 and lag2 for the logistic regression model
glm.fit <- glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred == Direction.2005)
# We can attach the Smarket data set for convenience
attach(Smarket)
# In the previous model, we found that the lag1 and lag2 are the best predictors in the model
# Let's build a smaller model with just lag1 and lag2 for the logistic regression model
glm.fit <- glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
# For better accuracy check, we are going to split our data into training and testing sets
# Make training and testing set
# Training data will be the data with year before 2005
train <- Year<2005
# In the previous model, we found that the lag1 and lag2 are the best predictors in the model
# Let's build a smaller model with just lag1 and lag2 for the logistic regression model
glm.fit <- glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
# Compare the prediction with the testing data
Direction.2005 <- Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
summary(glm.fit)
Smarket
table(glm.pred,Direction.2005)
mean(glm.pred == Direction.2005)
# Import the libraries for analysis
library(randomForest)
# In this section, we are going to use the tree-based methods for regresssion and classification.
# We first need to install the 'randomForest' and 'ISLR' packages
install.packages("randomForest")
install.packages("ISLR")
# Import the libraries for analysis
library(randomForest)
library(ISLR)
library(tree)
library(MASS)
install.packages("tree")
library(tree)
library(MASS)
# We are going to use the Boston data set from MASS library
?Boston
set.seed(1)
NumberofObservations = dim(Boston)[1]
SplitofTrainTest = 0.5 #let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Boston[train,]
testingData  = Boston[test,]
Testing_outcome = Boston$medv[test]
?randomForest
#Bagging is simply a special case of random forest with m = p (All predictors are used to grow trees)
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE)
install.packages("gbm")
#mtry is the nymber of variables randomly sampled as candidates at each slpit. The defult is sqrt(p) in classification and p/3 in regression
#The defult number of trees is 500
bag.boston
names(bag.boston)
summary(bag.boston)
importance(bag.boston)
varImpPlot(bag.boston)
predict.bag = predict(bag.boston,newdata = testingData)
plot(predict.bag,Testing_outcome)
abline(0,1)
MSE.bag = mean((predict.bag - Testing_outcome )^2)
MSE.bag
bag.boston = randomForest(medv~., data=trainingData, mtry=13, importance=TRUE, ntree=25)
predict.bag = predict(bag.boston, newdata=testingData)
MSE.bag.25Trees = mean((predict.bag - Testing_outcome )^2)
MSE.bag.25Trees #Our error increased
MSE.Rf=rep(0,13)
for(d in 1:13){
rf.boston = randomForest(medv~.,data = trainingData,mtry=d,importance = TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[d] = mean((predict.rf- Testing_outcome )^2)
}
for(d in 1:13){
rf.boston = randomForest(medv~., data=trainingData, mtry=d, importance=TRUE)
predict.rf = predict(rf.boston,newdata = testingData)
MSE.Rf[d] = mean((predict.rf- Testing_outcome )^2)
}
MTRY = c(1:13)
plot(MTRY,MSE.Rf,type="b",col="red")
min(MSE.Rf)
#6 created the minimum error - if you repeat this over and over you may get another miminizers such as 4 or 5
rf.boston = randomForest(medv~., data=trainingData, mtry=6, importance=TRUE)
predict.rf = predict(rf.boston, newdata=testingData)
MSE.Rf = mean((predict.rf- Testing_outcome )^2)
MSE.Rf
importance(rf.boston)
varImpPlot(rf.boston)
########################
######  Boosting #######
########################
install.packages("gbm")
library(gbm)  #You must add this library
set.seed(1)
?gbm
#If you are running regression problems then use distribution = "gaussian". If you are working on
#binary classfiication problems, then use distribution = "bernoulli"
#The default value of Lambda is 0.001
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4)
boost.boston
summary(boost.boston)
summary(Boston$medv)
#Parial dependence plots for rm and lstat
#These plots illustrate the marginal effect of the selected variables on the response (medv) after
#integerating out the other variables. as we expect medv is increasing with rm and decreasing with lstat
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
mean((predict.boost-Testing_outcome )^2)
#Let's Change Lambda
Lambda = c(.00001,0.0001,0.001,.01,0.1,.15,.2)
Counter = 1
MSE.Boost = rep(0,7)
for(d in Lambda){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=5000, interaction.depth=4, shrinkage=d)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=5000)
MSE.Boost[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# Plotting the MSE for each Lambda
plot(Lambda,MSE.Boost,type="b",col="red")
min(MSE.Boost)
#Now let's fix Lambda and change size of the tree
TreeSize = c(50,100,200,400,500,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000)
Counter = 1
MSE.Boost.Tree = rep(0,15)
for(d in TreeSize){
boost.boston = gbm(medv~., data=trainingData, distribution="gaussian", n.trees=d, interaction.depth=4, shrinkage=0.01)
predict.boost = predict(boost.boston, newdata=testingData, n.trees=d)
MSE.Boost.Tree[Counter] = mean((predict.boost- Testing_outcome )^2)
Counter = Counter + 1
}
# The miminum happened at Lambda = 0.01
plot(TreeSize,MSE.Boost.Tree,type="b",col="red")
# it seems like 5000 was a very good choice
min(MSE.Boost.Tree)
# Remove all memories in the global environment
rm(list=ls(all=TRUE))
# Loading the required libraries for the lab
require(ISLR)
attach(Wage)
# In this example, we are tyring to predict wages using age from the dataset
# First fit the model using the basic command for polynomial regression
fit.Poly.Basic = lm(wage~age+I(age^2)+I(age^3)+I(age^4),data = Wage)
summary(fit.Poly.Basic)
# We can also use the poly() function to fit the same model
fit.Poly =lm(wage~poly(age,4,raw=TRUE),data = Wage)
summary(fit.Poly)
fit.Poly =lm(wage~poly(age,4),data = Wage)
summary(fit.Poly)
# We can also use the poly() function to fit the same model
fit.Poly =lm(wage~poly(age,4,raw=TRUE),data = Wage)
summary(fit.Poly)
# Note that if we do not define raw=TRUE, the model will still have the same fitted values
# However, the coefficients for each term cannot be interpreted as usual
fit.Poly.Orth =lm(wage~poly(age,4),data = Wage)
summary(fit.Poly.Orth)
# Let's create a grid of valuesfor age for prediction
agelims = range(age)
age.grid=seq(agelims[1],to=agelims[2])
preds = predict(fit.Poly,newdata = list(age=age.grid),se = TRUE)
names(preds)
se.bands = cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
# Plot the data dn add the fit from the degree-4 polynomial
plot(age,wage,col="darkgray")
lines(age.grid,preds$fit,lwd=2,col="blue") #lwd is linewidth
matlines(age.grid,se.bands,col="blue",lty=2)  #lty shows type of line lty = 2 is used for broken lines
#### Using Anova in Nested sequence of Models ####
fita = lm(wage~education,data = Wage)
fitb = lm(wage~education + age,data = Wage)
fitc = lm(wage~education + poly(age,2),data = Wage)
fitd = lm(wage~education + poly(age,3),data = Wage)
fite = lm(wage~education + poly(age,4),data = Wage)
anova(fita,fitb,fitc,fitd,fite)
#### Logistic Regression ####
fit.Logistics = glm(I(wage>250)~poly(age,3), data=Wage, family=binomial)
summary(fit.Logistics)
preds.Logistics = predict(fit.Logistics, newdata=list(age=age.grid), se=TRUE)
names(preds.Logistics)
se.bands = preds.Logistics$fit+cbind(fit=0, lower=-2*preds.Logistics$se.fit, upper=2*preds.Logistics$se.fit)
se.bands[1:5,]
#Convert logistics to probabilities
prob.bands = exp(se.bands)/(1+exp(se.bands))
prob.bands[1:5,]
matplot(age.grid,prob.bands,col="blue",lwd=c(2,1,1),lty=c(1,2,2),type="l")
#### Fitting step functions ####
# In order to fit a step function, we need to use the cut() function
?cut
table(cut(age,4))
fit.Step = lm(age~cut(age,4),data=Wage)
summary(fit.Step)
#### Splines ####
library(splines)
library(ISLR)
#The bs() function generates the entire matrix of baseis functions for splines
#with the specified set of knots. By default the cubic splines are produced
attach(Wage)
### GAMS ####
install.packages("gam")
library(gam)
# Remove or clear all memories in the global environment
rm(list=ls(all=TRUE))
# book's Libraray
library(ISLR)
# This libraray is essential to fit decision trees
library(tree)
# The dataset is from ISLR Library
attach(Carseats)
# let's explore the dataset
head(Carseats)
# 400 observations 11 variables
dim(Carseats)
summary(ShelveLoc)
# you can set.seed to any number
set.seed(2)
NumberofObservations = nrow(Carseats)
NumberofObservations # that is 400
SplitofTrainTest = 0.5 # let's split the data 50/50
train = sample(1:NumberofObservations,NumberofObservations*SplitofTrainTest)
test = -train
trainingData = Carseats[train,]
testingData  = Carseats[test,]
Testing_outcome = Sales[test] # The is the our test outcomes
